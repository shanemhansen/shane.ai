#+hugo_base_dir: ../

* DONE CGO Performance In Go 1.21
CLOSED: [2023-09-01 Fri 14:42]
:PROPERTIES:
:EXPORT_FILE_NAME: cgo-performance-in-go1.21.org
:PRJ-DIR: ~/src/cgobench/
:END:

** Tl;Dr

Cgo calls take about 40ns, about the same time ~encoding/json~ takes to parse a single digit integer. On my 20 core machine
Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.

** Disclaimer

While alot of this article argues that "Cgo performance is good actually", please don't take that to mean "Cgo is good actually". I've
maintained production applications that use Cgo and non-trivial bindings to lua. Performance was great. Go upgrades were a regular source of toil.
The drawback to using Cgo is losing Go cross compilation benefits and having to manage c dependencies. These days
I mainly use Cgo for compatibility and to access libraries that happen to be written in C/C++.

** Cgo & performance
Cgo performance is poorly understood in Go, and searching for information online
mixes content from 2 years ago with content from 7 years ago. Cockroach labs wrote a [[https://www.cockroachlabs.com/blog/the-cost-and-complexity-of-cgo/][great article]]
that measured performance and touched on the complexities of using Cgo. Since then Go performance has improved quite a but, but everything else they said is relevant.
My similar benchmarks are 17x faster than what Cockroach labs saw in 2015. Some of that might be hardware by suspect most of it is just improvements
to Go. Unfortunately I see alot of Go programmers have internalized that "Cgo is slow" without really knowing
what it's slow compared to. Cgo is slow compared to a regular function call. It's certainly not slow
compared to doing any sort of I/O or parsing work.

In this post
I want to build on the idea of "[[https://gist.github.com/jboner/2841832][latency numbers every programmer should know]]" to figure
out where in the context of "slow" Cgo lands in the heirarchy of
L1 cache reference -> mutex lock -> main memory reference -> sending a packet on the network. These numbers
are from 2012 so they are really just here to give us a sense of scale:

| latency comparison numbers         |                          |
| L1 cache reference                 | 0.5 ns                   |
| Branch mispredict                  | 5   ns                   |
| L2 cache reference                 | 7   ns                   |
| Mutex lock/unlock                  | 25   ns                  |
| Main memory reference              | 100   ns                 |
| Compress 1K bytes with Zippy       | 3,000   ns        3 us   |
| Send 1K bytes over 1 Gbps network  | 10,000   ns       10 us  |
| Read 4K randomly from SSD*         | 150,000   ns      150 us |
| Read 1 MB sequentially from memory | 250,000   ns      250 us |
| Round trip within same datacenter  | 500,000   ns      500 us |

My thesis is: Cgo is has overhead, but it doesn't have as much overhead as it used to and it
may not have as much overhead as you think.

Lets talk about what Cgo is and a teeny bit about how it works. Cgo
is essentially Go's ffi. When you use Cgo you can call C functions from Go and
pass information back and forth (subject to some [[https://pkg.go.dev/cmd/cgo][rules]]). The Go compiler autogenerates some
functions to bridge between the Go & C and handle
things like differences in platform calling conventions. There are also mismatches in how blocking
calls are handled and how stack is allocated that make it impractical/unsafe to run Go and C
code on the same stack. I won't go too much into the implementation but at a high level "Cgo means IPC between threads"
is a good mental model.

** Benchmarking

Lets write some benchmarks to explore performance . You can follow along at [[https://github.com/shanemhansen/cgobench][github.com/shanemhansen/cgobench]]. The code in the repo is autogenerated
from the [[https://github.com/shanemhansen/shane.ai/blob/main/docs/content-org/all-posts.org][source org]] file for this article using an implementation of Knuth's literate programming. Is it the most productive way to write articles?
Probably not, but it's fun and frankly playing around with new workflows helps my ADHD brain focus. But I digress.

First off we'll put a no-op go function in ~bench.go~ and do a parallel benchmark. It doesn't do anything
which is a great place to start.

~bench.go~
#+NAME: Call
#+begin_src go :main no :package cgobench
func Call() {
	// do less
}
#+end_src

#+RESULTS:

Now we'll add a simple parallel benchmark helper along with our empty call benchmark. I'm going to start out
with something so simple the compiler can inline and then compare that to a non-inlined call. When comparing
Go vs Cgo it's important to realize that the Go compiler can't inline Cgo functions.

~bench_test.go~
#+NAME: BenchmarkCall
#+begin_src go :main no :package cgobench :imports testing
// helper to cut down on boilerplate
func pbench(b *testing.B, f func()) {
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			f()
		}
	})
	
}
// Same as above, but explicitly calling the inlineable Call func.
func BenchmarkEmptyCallInlineable(b *testing.B) {
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			Call()
		}
	})
}
func BenchmarkEmptyCall(b *testing.B) {
	pbench(b, Call)
}
#+end_src

In the case of benchmarking no-ops it's always good to check and make sure your code didn't get completely optimized away. 
I tend to just look at the disassembled output in ~BenchmarkEmptyCall~ and sure enough I see a convincing ~call *%rax~ instruction in the assembly. A non dynamic
dispatch version would look like: ~call foo+0x3~ but this version is calling a function who's address is in the rax register.

Let's compile and examine:

#+begin_src sh :dir (org-entry-get nil "PRJ-DIR" t) :results verbatim :exports output
go test -c
objdump -S cgobench.test  | grep -A15 '^0.*/cgobench.BenchmarkEmptyCall.pbench.func'
#+end_src

#+RESULTS:
#+begin_example
0000000000522920 <github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1>:
	b.RunParallel(func(pb *testing.PB) {
  522920:	49 3b 66 10          	cmp    0x10(%r14),%rsp
  522924:	76 36                	jbe    52295c <github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x3c>
  522926:	55                   	push   %rbp
  522927:	48 89 e5             	mov    %rsp,%rbp
  52292a:	48 83 ec 18          	sub    $0x18,%rsp
  52292e:	48 89 44 24 10       	mov    %rax,0x10(%rsp)
  522933:	48 8b 4a 08          	mov    0x8(%rdx),%rcx
  522937:	48 89 4c 24 08       	mov    %rcx,0x8(%rsp)
		for pb.Next() {
  52293c:	eb 0f                	jmp    52294d <github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x2d>
			f()
  52293e:	48 8b 54 24 08       	mov    0x8(%rsp),%rdx
  522943:	48 8b 02             	mov    (%rdx),%rax
  522946:	ff d0                	call   *%rax
#+end_example

Now that we've verified our benchmark we can run it. I'm going to run benchmarks with a few different coure count values so we can see how the output changes. While writing this
post I experimented with some other values and for most benchmarks performance increased linearly with core count up to 16 before it began falling off.
On my machine with 20 cores the overhead of the dynamic call is around 1ns and the inlinable version is significantly faster. As
expected.
#+begin_src sh :dir (org-entry-get nil "PRJ-DIR" t) :results verbatim :exports output
go test -cpu=1,2,4,8,16  -bench EmptyCall
#+end_src

#+RESULTS:
#+begin_example
goos: linux
goarch: amd64
pkg: github.com/shanemhansen/cgobench
cpu: 12th Gen Intel(R) Core(TM) i7-12700H
BenchmarkEmptyCallInlineable       	1000000000	         0.2784 ns/op
BenchmarkEmptyCallInlineable-2     	1000000000	         0.1383 ns/op
BenchmarkEmptyCallInlineable-4     	1000000000	         0.07377 ns/op
BenchmarkEmptyCallInlineable-8     	1000000000	         0.04089 ns/op
BenchmarkEmptyCallInlineable-16    	1000000000	         0.02481 ns/op
BenchmarkEmptyCall                 	718694536	         1.665 ns/op
BenchmarkEmptyCall-2               	1000000000	         0.8346 ns/op
BenchmarkEmptyCall-4               	1000000000	         0.4443 ns/op
BenchmarkEmptyCall-8               	1000000000	         0.2385 ns/op
BenchmarkEmptyCall-16              	1000000000	         0.1399 ns/op
PASS
ok  	github.com/shanemhansen/cgobench	3.819s
#+end_example

So now I can think of "go function call" cost as "a little more expensive than a L1 cache reference" in the above table.
What happens if we add a Cgo call?

Below is a trivial c function to add 2 integers and a go function to call it. Note that although we might
expect gcc to inline trivial_add, we don't expect Go's compiler to. I did play with some even simpler
C functions but they didn't really perform better.

~bench.go~
#+NAME: cgoDefinition
#+begin_src c
int trivial_add(int a, int b) {
  return a+b;
}
#+end_src

#+NAME: CallCgo
#+begin_src go
// wow this is easy
// import "C"
func CgoCall() {
	C.trivial_add(1,2)
}
#+end_src

~bench_test.go~
#+NAME: BenchmarkCgo
#+begin_src go
func BenchmarkCgoCall(b *testing.B) {
	pbench(b, CgoCall)
}
#+end_src

We run benchmarks in the usual way. Single threaded Cgo overhead is about 40ns. Performance
seems to scale linearly with the number of cores up to 16ish so if I had a Cgo-bound workload I might not
bother putting it on a machine with 32 core, but real workloads usually involve more than just calling a cgo func. We can see:

- Cgo has 40ns overhead. That sits somewhere between "mutex lock" and "main memory reference".
- 40ns/op is 25 million ops/s. That's pretty good for most projects I've worked on. At 4ns/op and 16 cores we're getting 250 million ops/s.
   
#+begin_src sh :dir (org-entry-get nil "PRJ-DIR" t) :results verbatim :exports output
go test -cpu=1,2,4,8,16,32  -bench Cgo
#+end_src

#+RESULTS:
#+begin_example
goos: linux
goarch: amd64
pkg: github.com/shanemhansen/cgobench
cpu: 12th Gen Intel(R) Core(TM) i7-12700H
BenchmarkCgoCall       	28711474	        38.93 ns/op
BenchmarkCgoCall-2     	60680826	        20.30 ns/op
BenchmarkCgoCall-4     	100000000	        10.46 ns/op
BenchmarkCgoCall-8     	198091461	         6.134 ns/op
BenchmarkCgoCall-16    	248427465	         4.949 ns/op
BenchmarkCgoCall-32    	256506208	         4.328 ns/op
PASS
ok  	github.com/shanemhansen/cgobench	8.609s
#+end_example

Now I want to understand a little bit more about why performance is that way. We'll use Go's great profiling tools to get a better picture of performance at higher core counts. I'm a fan of the pprof web view,
which tells us that ~runtime.(*timeHistorgram).record~ and ~runtime.casgstatus~ are taking lots of time. This
tracks with [[https://groups.google.com/g/golang-dev/c/XSkrp1_FdiU?pli=1][Ian Lance Taylor's observations]]. Interestingly he doesn't expect these operations to be contended,
so there's potential for improving performance.

Running the test and collecting results:
#+begin_src sh :dir (org-entry-get nil "PRJ-DIR" t) :results verbatim :exports output
go test -c 
./cgobench.test  -test.cpuprofile=c.out -test.cpu=16 -test.bench Cgo
go tool pprof -png cgobench.test c.out > cpu.png
#+end_src

#+RESULTS:
: goos: linux
: goarch: amd64
: pkg: github.com/shanemhansen/cgobench
: cpu: 12th Gen Intel(R) Core(TM) i7-12700H
: BenchmarkCgoCall-16    	235322289	         4.955 ns/op
: PASS

Note the 2 large boxes near the bottom:
#+attr_html: :style width:50%
[[file:/cpu.png]]

I also use linux ~perf~. It does a good job of being able to profile cross language stuff for compiled languages as well as combining both userspace and kernel performance info.
A quick snapshot of (one of) the hot instructions in question in question from perf:

[[file:/casgstatus.png]]

Before we put it all together I'll add one final piece of data in to help us get perspective. Here's a carefully crafted JSON decoding benchmark that just parses an integer. It's
written using ~json.NewDecoder~ because just ~json.Unmarshal~ allocates too much. What you'll see below is that a Cgo call is 20% cheaper than a trivial JSON parse using the standard
library in both single threaded and parallel tests.

~bench_test.go~
#+NAME: BenchmarkJSON
#+begin_src go :main no :package cgobench :imports testing
func BenchmarkJSONCall(b *testing.B) {
	msg := `1`
	b.RunParallel(func(pb *testing.PB) {
		var dst int
		r := strings.NewReader(msg)
		dec := json.NewDecoder(r)
		for pb.Next() {
			r.Seek(0, io.SeekStart)
			if err := dec.Decode(&dst); err != nil {
				panic(err)
			}
		}
	})
}
#+end_src

#+RESULTS: BenchmarkJSON

#+begin_src sh :dir (org-entry-get nil "PRJ-DIR" t) :results verbatim :exports output
go test -cpu=1,16 -bench JSON
#+end_src

#+RESULTS:
: goos: linux
: goarch: amd64
: pkg: github.com/shanemhansen/cgobench
: cpu: 12th Gen Intel(R) Core(TM) i7-12700H
: BenchmarkJSONCall       	21399691	        52.79 ns/op
: BenchmarkJSONCall-16    	217874599	         5.471 ns/op
: PASS
: ok  	github.com/shanemhansen/cgobench	2.942s

** Conclusions

So at this point we've measured performance overhead of Cgo, at least
in terms of wall clock time (note that we haven't looked at memory/thread count/battery usage/etc). We know that the overhead is on the order of 2 mutex operations and that it does
scale with number of cores up to around 16. We've also seen that with 16 cores we can do around 4ns/op or close to 250 million Cgo ops/s. So if I was looking at using Cgo in 2023 I'd definitely use
it outside of very hot loops. There's many reasons I wouldn't use Cgo in 2023 (see disclaimer), but performance is unlikely to be one of them.

I'll end with this little Cgo version of "latency numbers every programmer should know" table:


| Go/Cgo latency          |          |            |
| Benchmark Name          | 1 core   | 16 cores   |
| Inlined Empty func      | 0.271 ns | 0.02489 ns |
| Empty func              | 1.5s ns  | 0.135 ns   |
| =cgo=                   | =40 ns=  | =4.281 ns= |
| encoding/json int parse | 52.89 ns | 5.518 ns   |



#+begin_src go :main no :exports none :package cgobench  :noweb tangle :tangle (concat (org-entry-get nil "PRJ-DIR" t) "bench.go")
/*
#include <time.h>
<<cgoDefinition>>
,*/
import "C"
<<Call>>
<<CallCgo>>
<<CallCgoSleep>>
#+end_src
#+begin_src go :main no :exports none :package cgobench  :noweb tangle :tangle (concat (org-entry-get nil "PRJ-DIR" t) "bench_test.go")
import (
	"testing"
	"encoding/json"
	"strings"
	"io"
)
<<BenchmarkJSON>>
<<BenchmarkCall>>
<<BenchmarkCgo>>
<<BenchmarkCgoSleep>>
#+end_src

#+RESULTS:

#+begin_src elisp :exports none
(org-babel-tangle)
(org-hugo-export-wim-to-md)
#+end_src

#+RESULTS:
: /home/shane/src/shane.ai/docs/content/posts/cgo-performance-in-go1.21.md



* DONE Some notes on javascript jit and deopt
CLOSED: [2023-09-26 Tue 13:34]
:PROPERTIES:
:EXPORT_FILE_NAME: notes-on-javascript-jit-deopt.org
:END:

Recently I read a fantastic article walking through jit optimizations
and how changes to source code could impact those: [[https://www.recompiled.dev/blog/deopt/][Side effecting a deopt]].

As I shared it with folks, a few of them had some questions about low level optmizations in general and
I wrote this as a little explainer for people who are interested in learning more about how
javascript runtimes can model/compile/jit/execute their js code. So I wrote this explainer
to go along with the original article. Important: please read the original article first or have it pulled up next to this
article.

My goal is that by the time we're done the reader understands a bit more about:

1. How their computer can model arbitrary property/value pairs (~Objects~)
2. How their computer can model ~Objects~ with fixed properties (what many languages would call a struct, or even a class)
3. Some basics about how a javascript engines can observe how a value runs through the system


** Understanding hexadecimal notation and RAM

Many programs that deal with memory addresses use hexadecimal. So instead of saying "20th byte" they say "0x14". Base 16 just adds 6 extra "digits" and uses
a-f to represent them. So here's a few numbers in hex and base 10. We often write a prefix ~0x~ for hex numbers to let you know they aren't base 10.

| base 10 | hex |
|       1 |   1 |
|      10 |   a |
|      11 |   b |
|      16 |  10 |
|      17 |  11 |
|      20 |  14 |

For those who like a more theoretical description: the idea is that any number we work with like ~123~ really means: ~1*10^2+2*20^1+3*10^0~. If you look at our 0x14 example that means:
~0x14 = 1*16^1+4*16^0 == 20~.

About the only time I mention hex is when discussing the output of optimization tools and it's really simple
because we'll deal with things like ~0x2~ which is 2. And ~0xc~ which is 13.



** Disclaimers:

I'm going to be using the madeup phrase "RAM indexes". The real world calls "RAM indexes" pointers. I'm
hopeful that using "RAM index" as if RAM is an array of bytes is clearer for the target audience. But feel free
to translate statements like ~RAM[foo]~ to ~*foo~. Similarly I will talk alot about records that are packed,
the normal industry nomenclature would probably be ~C struct~ (with some caveats around packing, padding, field ordering, etc).
I'm pretending in this example that our computers have byte sized words and that ASCII is great because
32bit/64bit and utf8/utf16 don't add anything to this post and we'd have to count in multiples of 4 or 8.
Finally my hashtables aren't fast at all. I literally just want people to conceptualize the basic idea of hashing a key to find a bucket
as an alternative to some sort of linear search.


** How to represent data in RAM

The atomic unit of data we'll talk about today is a byte. It's a series of 8 ones and zeros. Or it's a number between 0 and 255 (because ~2^8=256~).

If we want to model some sort of record on the computer, such as a person who has a numeric id and a numeric age, we have to come up with a way of
representing those objects in memory and referring to them. Most of the readers here understand what I mean when I say something like ~let Person = {id:0, age:24}~, but computers don't.
The simplest way to represent a person "object" then is as 2 bytes next to each other. The first one is their id. The second one is their age. Let's write out
an array of 2 persons in RAM:

~let persons = [{id:1, age:24}, {id:2, age:28}]~

| RAM index  |  0 |   1 |  2 |   3 | 4 | 5 | 6 | 7 | 8 |
| RAM value  |  1 |  24 |  2 |  28 | 0 | 0 | 0 | 0 | 0 |
| Field name | id | age | id | age |   |   |   |   |   |

So now if you tell a computer where the object starts (via pointer or RAM index), it knows the ~id~ is at offset 0, and the ~age~ is at offset 1. It knows the size of a ~person~ record (2 bytes).
If it needs to operate on those values (LOAD/STORE/MOV low level assembly instructions)
it can directly write machine code that uses those offsets. Let's write some psuedocode for returning the age of the 2nd person that somewhat mirrors the actual instructions your computer
executes.

#+begin_src js
let persons_array = ;// some number that is an index into RAM
let size_of_person = 2;
let age_offset = 1;
person_2_age = RAM[
    persons_array +// where the array starts
	size_of_person +// skip ahead 1
	age_offset // age is 2nd value
]; 
// or equivalantly: one addition. one ram lookup.
person_2_age = RAM[persons_array+3];
#+end_src

Let's show one more example for strings. Unfortunately strings are variable length. So if you have a record like ~let person = [{id:"id2", age:29}, {id:"id10", age:40}]~ those
strings can't be packed into the same orderly layout as above where each field starts at a fixed offset. We typically store the strings somewhere else and store the RAM index of that location in the person record. Let's draw that out: because these
strings are variable length we use a special marker character called null or ~\0~ to indicate end of string. This is how the C programming language represents strings historically.

| RAM index  |  0 |   1 |  2 |   3 | 4 | 5 | 6 |  7 | 8 |   |   |   |    |
| RAM value  |  4 |  29 |  8 |  30 | i | d | 2 | \0 | i | d | 1 | 0 | \0 |
| field name | id | age | id | age |   |   |   |    |   |   |   |   |    |


The big thing to notice here is the RAM value for the ~id~. ~4~ and ~8~. Those correspond to RAM index ~4~ and ~8~. Saying "here's a fixed sized number who's value is where you can find the variable
sized data". So now you have enough background to understand a really basic record/object/struct type. This is an idealized example, in the real world the actual layout can be more complicated but
for our purposes today this is a good enough mental model of "how to layout an object in memory to make it easy for the CPU to get your data".

** Hashtable basics

Now the problem is that the above techniques do not allow us to deal with arbitrary field names. We can't add fields later on or that would mess up all the calculations like "load the 2nd byte of the
record to get the ~age~". Javascript ~Objects~ allow any number of property/value pairs (in the context of hashtables properties are called keys) and people need to quickly look up a key when they write ~x.foo~. Let's sketch a couple ways people could store those in memory.
The simplest method is to put all the values right next to each other one after another and use ~\0~ to separate them. Here's a way to encode ~{"key1":"value1", "key2":"value2"}~

| RAM index | 0 | 1 | 2 | 3 |  4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 |
| RAM value | k | e | y | 1 | \0 | v | a | l | u | e |  2 | \0 |  k |  e |  y |  2 | \0 |  v |  a |  l |  u |  e |  2 | \0 |
So if we stored data this way we could pretty easily write some psuedocode for looking up a key/value pair. Just compare the current key in memory with the
property name you are looking for byte by byte. If it doesn't match, skip the next value until you find a null, and start over. Unfortunately this would be slow because
the farther along your key is, the longer you have to search. So we have something called hashtables. The basic idea of a hashtable is we have some function to convert our
keys to numbers. Then we convert those numbers to a bucket index. A hashtable has several fixed sized buckets right next to each other in RAM. So I might have 10 buckets and when I store "foo" I run a
function which says hash("foo") = some random number like 12313213
and then I convert that to a bucket index by saying ~12313213%10 = 3~. Then in bucket number 3 I store something like: ~foo\0value\0~ (well actually it's fixed sized. In bucket number 3 I store the RAM index of some other piece of memory that holds ~foo\0value\0~. The great thing about this
is that as long as there are no collisions (2 different keys that go to the same bucket) I can find any property without looking at all the data. The overhead for finding a value in a hashtable
without collisions just depends on how long it takes to run ~hash(key)~ and not on the number of keys in the hashtable. I can handle collisions (multiple properties
ending up in the same bucket) by just using the strategy listed above and smashing key value pairs together. Without going into too much detail, real world hashtables have better strategies for minimizing collisions by detecting
when they are close to full and resizing. Sometimes instead of a long chain of key/value pairs they have another hashtable, or a tree. But for today let's try and show an example of how a 3 key value pairs might be represented in a simple 2 bucket hashtable. The example record is: ~let person = {name: "bob", id: "id3", age:"21"}~ I'm going to
draw this as a graph and then also as a linear array of bytes if we just append key/value pairs when there's a collision.

#+begin_src dot :file hashtable.png
digraph G {
	label="Hashtable with 2 buckets and chaining";
	    node [shape=record];
	    subgraph cluster_0 {
	    	     label="buckets";
	    	     hashtable [label="<f0> 0|<f1> 1"];
	    }
	    hashtable:f0->ram:f0;
	    hashtable:f1->ram:f16;
	    ram [label="<f0> n|<f1> a|<f2> m|<f3> e|<f4> \\0|<f5> b|<f6> o|<f7> b|<f8> \\0|<f9>a|<f10> g|<f11> e|<f12> \\0|<f13> 2|<f14> 1|<f15> \\0|<fa> ...|<f16> i|<f17> d|<f18> \\0|<f19> i|<f20> d|<f21> 3|<f22> \\0"];
} 
#+end_src

#+RESULTS:
[[file:hashtable.png]]


Here's a table of numbers for representing the same thing. We have a person value which consists of a bucket count and a buckets RAM index. The bucket's RAM index
is where the bucket list starts. Next up we have the 2 buckets. In the real world all of this wouldn't be packed so close. Which is important for adding more buckets
and more key/values. Our first bucket points to index 4, which is where they key/value pair "name:bob" is stored, etc.

| RAM index |           0 |              1 |         2 |         3 | 4 | 5 | 6 | 7 |  8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 |
| RAM value |           2 |              2 |         4 |        20 | n | a | m | e | \0 | b |  o |  b | \0 |  a |  g |  e | \0 |  2 |  1 | \0 |  i |  d | \0 |  i |  d |  3 | \0 |
| field     | bucketCount | bucketRamIndex | bucket[0] | bucket[1] |   |   |   |   |    |   |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |

So this should give you a basic idea that looking up a hashtable key stays fast as you add more items, as long as you don't have alot more items than buckets. But now instead of the
cpu being able to translate something like ~person.name~ into a fixed offset from person, it must instead do the following psuedocode.
#+begin_src js
// imaginary ram already defined
let RAM = [];
// RAM index where person is located.
let person = 0;
// we've defined it to be the 1st element
let bucketCountOffset = 0;
//we've deinfed it to be the 2nd element
let bucketRamIndexOffset = 1;
// the size of the bucket would have to be stored
let bucketCount = RAM[person+bucketCountOffset]
// hash is defined elsewhere. It takes a string and returns an integer.
let bucketNumber = hash("person")%bucketCount 
let startOfbuckets = RAM[person+bucketRamIndexOffset]
let bucketRAMIndex=RAM[startOfBuckets+bucketNumber]
// search_bucket searches a chain of "key\0value\0key\0value\0" pairs.
let age = search_bucket(bucketRAMINDEX, "name"); 
#+end_src

#+RESULTS:

Note that this requires us to look at several bytes in the property name, do some math, and load several values from RAM just to get to where age is stored. Going into the latency
of all these operations is beyond the scope here, but it's safe to say that two things that often make code slow are:

1. Searching for values without any ordering/index
2. A bunch of chained RAM lookups such as the 3 RAM lookups we had to do, one of which was chained (often referred to as pointer chasing).


** Optimizing a hashtable

So now the optimizations javascript runtimes want to do start to make sense. Any js ~Object~ *could* be a full hashtable, but since so often
js Objects have fixed property names, the runtime would prefer to represent them similar to the 2 byte packed format we used above for person. In fact
it turns out that this happens pretty often. After all our example above where we store a list of objects with the same structure comes up pretty often.

Javascript runtimes detect these patterns and optimize the storage layout and data access. It's alot harder than you'd expect because javascript is such
a dynamic language. So not only must they make the optimizations based on assumptions like "nobody will add keys to this object", but they must keep track
of when those assumptions are violated and fall back to regular hashtables or unoptimized code. They have to do this entire dance on a time/memory budget because
afterall they are trying to speed up your code so if their transformations and untransformations aren't fast enough, they've failed at their purpose.

So a good mental model is "javascript runtimes look at data access and speculatively optimize the storage, while maintaining a list of assumptions and dependencies that may
invalidate their assumptions. They might have to deoptimize/reoptimize if one of those assumptions changes".

** A simple example to optimize

Let's look at a javascript function from the blog:

#+begin_src js
const x = { foo: 1 };

function load() {
	return x.foo;
}
console.log(load());
#+end_src

Now just by looking at this function it's pretty easy to see that in the absence of "funny business"
(certain javascript dynamic features that can cause weird things to happen) it seems like our
entire program could easily be reduced to ~console.log(1)~. Let's trace through how a js engine
might execute each version of the program.

*** Execution trace

1. Create a new string value ~"foo"~
2. Create a new number value ~1~
3. Create a new Object and store the above key/value pair in it (which involves hashing and finding a bucket)
4. Call the ~load~ function.
5. Find a variable named ~x~ (note that because it's not defined in the current function sometimes we might actually does have to "look farther" to notice ~x~ is a global variable).
6. Lookup the value of the ~foo~ property on ~x~. This could involve hashing the string ~"foo"~ to some number to get a bucket to seach for a value.
7. Return that value ~1~
8. Call ~console.log()~ with the value ~1~

I'm intentionally trying to omit some boring details while giving you an idea of the work the CPU is doing. What's important if we're trying to optimize the performance of ~load~
is to notice that we're probably doing alot of work hashing the foo key to find where ~1~ is stored. But we're smart and we looked at the code and decided that we can replace all the above steps
with just the last one : call ~console.log()~ with the value ~1~. And that is why in the [[https://www.recompiled.dev/blog/deopt/][referenced blog post]] the author points out that the runtime debug information printed out:

#+begin_src sh
0x280008150   130  d2800040       movz x0, #0x2
// The value is 0x2, and not 0x1 because it's been tagged.
#+end_src

Don't worry about anything but 0x2. That statement is essentially the proof that the optimizer reduced our function call and our object property lookup
down to just the value ~1~. Javascript uses something called tagged numbers so you'll have to trust me that in this case 2 on the computer means one in javascript. It's part
of how javascript distinguishes between a number and an ~Object~ at a certain RAM index.

Let's also look into what assumptions/observations the runtime made in order to make the assumption that
~console.log(load())~ is ~console.log(1)~. This is not an exhaustive list:

1. We have to assume that ~load()~ invokes the load function defined above, it can't have been overridden.
2. We have to assume that nothing has changed x.foo (the const means x is always the same object, not that the object is immutable)
3. We have to assume nobody has messed with the object's prototype using ~x.__defineGetter__("foo", function() { return 2})~

And there are some more dependencies that are an artifact of how the runtime implements optimization.


*** Causing a deopt by changing something that looked constant

So back to reading "side effecting a deopt": Now we understand why ~x.foo=5~ causes deoptimization/reoptimization.
What's fascinating is that in their examples v8 didn't fall back to a completely unoptimized path. Instead
of assuming load always returned 1, is fell back to ~LoadTaggedField(0xc, decompressed)~. I'm going
to be a little lazy and not dive deep into the implementation here, and instead assume that means
the code is essentially doing is analogous to what we've described as modeling the data for x as record
who's with one integer field. Let's write a human readable description of what the runtime has now optimized our function to.
I'll include an offset called foo_offset just like we did above
for person and age. I'm also
going to invent a new function. Typically in C this is called ~&~ for "address of" (or RAM index)
I'll call it ram_index().
So a psuedocode version of the new optimized code is below: we essentially load the foo property
from a fixed offset of the RAM index where x is stored.

#+begin_src js
let foo_offset=0
console.log(RAM[ram_index(foo)+foo_offset]);
// For the c folks
// console.log(*(&(foo+foo_offset)))
#+end_src

*** Expando
We can now pretty easily talk about the effects of taking our optimized value x and adding new properties:

#+begin_src js
x.__expando = 2;
#+end_src

This causes a deoptimization because the compiler went through all this trouble to figure out that
x had just one property that was a number. Now it has 2. So it has to decide whether it wants to
create a new optimized storage layout for x with just 2 numbers squished together or whether it
should hold off on optimizing. There's no one right answer, but clearly the previous work was invalidated.

*** Spooky action at a distance

The last example in the blog post is a fun one. They took our existing optimized code and added a new variable:

#+begin_src js
const y = {foo:1};
y.__expando = 4;
#+end_src

What's surprising is this code causes x to become deoptimized! There's no fundamental reason
why this has to be true but the short answer is that when the runtime optimizes these variables
it essentially has to store a schema (or hidden class) somewhere that says "foo is an integer property at at offset 0".

When the runtime sees ~y={foo:1}~ is does some deduplication so x and y both share the same hidden class/schema. There's now a dependency here
which is that.... x and y have the same properties.

When we add a new property to y, we break that assumption and x and y can no longer share the same hidden class. There
are lots of choices about what the runtime could do, and sorting that out results in another
round of deoptimization/reoptimization.

Let's draw a dependency graph showing the hidden class before/after.

#+begin_src dot :file deps.png
digraph G {
	node[shape=record];
	x [label="x|<f0> hiddenClass"];
	y [label="y|<f0> hiddenClass"];
	x:hiddenClass->class;
	y:hiddenClass->class [color="red" style="dashed"];
	class [label="<f0> foo|"];
	expando [label="<f0> foo|<f1> __expando"];
	y->expando [color="green"];
} 
#+end_src

#+RESULTS:
[[file:deps.png]]


*** Wrapping up

This is far from an exhausive post, I'm not even sure it was worth writing but so often things like compilers and jits are seen as magic,
which to be honest they kind of are, and I'm hopeful that this post helps people unfamiliar with C or jits start to build a mental model
of how the computer can map their code and data into something it understands.

There's no particular action item to take away. No simple way to make your code fast. In fact javascript runtimes contain so many
heuristics and tricks that sometimes the code that seems like it should run faster runs slower (for example there are many optimizations around
string building and the runtimes can often use fancy datastructures like ropes to speed things up).

If there's one performance takeaway I would give to the audience of this post, it's this:

Use a profiler. v8 supports writing symbol maps for linux perf to consume. Chrome has a ton of debug tools. Firefox has great trace viewing
infrastructure. Put your application under a magnifying class and try to understand where it's spending time. Even if your application is faster
than it needs to be it's still nice to know. Is it parsing JSON? Interacting with the DOM? Get a baseline so that as you collect new profiles and develop new features you can spot weird regressions.

And ask yourself if that makes sense. At my current job alot of programs spend alot of time parsing protocol buffers. This is fairly reasonable. At previous
jobs our caching edge ingress proxies spent alot of time gzipping uncacheable content and doing TLS. This was somewhat reasonable but pointed to opportunities
to improve the customer experience by making more content cacheable.

Hopefully you found some of this informative, now go out there and put an application under a magnifying glass. I guarantee you'll be surprised at what you find.


* A new to me strace feature
:PROPERTIES:
:EXPORT_FILE_NAME: new-to-me-strace-feature.org
:END:



I discovered ~strace~ somewhere between my first part time web development part time job in 2005
and my first full time "software engineering" job in 2008, and it seemed like a superpower giving me x-ray
vision into running infrastructure. When a process was stuck, or existing after a cryptic error message,
instead of grepping around I could get a pretty good timeline of what the process was up to.

After 15+ years I thought I knew most of the useful features, but last month I found a new incredibly useful flag: ~--stack-traces~. It appears
it was added in 2014 and I'm just finding out about it now! When you specify the ~--stack-traces~ flag strace will print the stack trace that
resulted in the system call. I was debugging some complex signal handling interactions between Go and Cgo and while I knew someone was messing
with signal I didn't know who. After a couple hours of grepping the entire codebase I just wasn't sure how the code ended up doing what it did.

But I did something that works surprisingly often on the internet: I imagined the feature I wanted existed and googled for it (or looked at the man page),
so after a quick look for "strace stack traces" I found the feature existed! When enabled it will cause ~strace~ to print stacktraces so you can see
how the code got where it got. Let's look at an example with wget and networking to just get a feel for how DNS resolution and TCP connections are handled
in a simple http request. (Note: I was using curl but honestly it did some su

There's alot of information so for today let's focus just on the system calls ~strace~ considers to be networking related. I've also
included a ~--status=successful~ flag to cut down on noise for this post.

See if you can spot the points below where strace:

- Attempts a DNS lookup over a unix domain socket
- Starts a thread and does  lookup to a local resolver
- Creates a TCP connection to shane.ai

#+begin_src sh :results verbatim
strace -f --stack-traces --status=successful  -e socket,connect,bind,recvfrom,sendmmsg wget -nv --max-redirect=0 'http://shane.ai/' > log 2>/dev/null 2>&1 || true
#grep -oP 'wget\(\) \[[^\]]+' log | sed -e 's/wget() \[//g' | addr2line -fp -e $(which wget)
cat log | head -3
# some post-processing to get the right symbols
#+end_src

#+RESULTS:
#+begin_example
socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x38b1) [0x169301]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x3f01) [0x169951]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x4397) [0x169de7]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x1dea) [0x16783a]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1b92) [0x103f42]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, 0) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x38b1) [0x169301]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x45d7) [0x16a027]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(svc_run+0x1f07) [0x167957]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1b92) [0x103f42]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
socket(AF_INET, SOCK_DGRAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_randomid+0x199) [0x1473a9]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_send+0x2bf) [0x147e6f]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_query+0x1c1) [0x145de1]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_search+0x472) [0x146ac2]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(_nss_dns_gethostbyname4_r+0x251) [0x13e781]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1ec0) [0x104270]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("127.0.0.53")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_randomid+0xfd) [0x14730d]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_send+0x2bf) [0x147e6f]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_query+0x1c1) [0x145de1]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_search+0x472) [0x146ac2]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(_nss_dns_gethostbyname4_r+0x251) [0x13e781]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1ec0) [0x104270]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
sendmmsg(3, [{msg_hdr={msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base="ll\1 \0\1\0\0\0\0\0\1\5shane\2ai\0\0\1\0\1\0\0)\4\260\0"..., iov_len=37}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, msg_len=37}, {msg_hdr={msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base="\342\222\1 \0\1\0\0\0\0\0\1\5shane\2ai\0\0\34\0\1\0\0)\4\260\0"..., iov_len=37}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, msg_len=37}], 2, MSG_NOSIGNAL) = 2
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__sendmmsg+0x17) [0x120627]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_send+0xb93) [0x148743]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_query+0x1c1) [0x145de1]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_search+0x472) [0x146ac2]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(_nss_dns_gethostbyname4_r+0x251) [0x13e781]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1ec0) [0x104270]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
recvfrom(3, "ll\201\200\0\1\0\4\0\0\0\1\5shane\2ai\0\0\1\0\1\300\f\0\1\0\1"..., 2048, 0, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("127.0.0.53")}, [28 => 16]) = 101
 > /usr/lib/x86_64-linux-gnu/libc.so.6(recvfrom+0x17) [0x11ff27]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_send+0x76c) [0x14831c]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_query+0x1c1) [0x145de1]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_search+0x472) [0x146ac2]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(_nss_dns_gethostbyname4_r+0x251) [0x13e781]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1ec0) [0x104270]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
recvfrom(3, "\342\222\201\200\0\1\0\0\0\1\0\1\5shane\2ai\0\0\34\0\1\300\f\0\6\0\1"..., 65536, 0, {sa_family=AF_INET, sin_port=htons(53), sin_addr=inet_addr("127.0.0.53")}, [28 => 16]) = 110
 > /usr/lib/x86_64-linux-gnu/libc.so.6(recvfrom+0x17) [0x11ff27]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_send+0x76c) [0x14831c]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_query+0x1c1) [0x145de1]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__res_context_search+0x472) [0x146ac2]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(_nss_dns_gethostbyname4_r+0x251) [0x13e781]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1ec0) [0x104270]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__idna_from_dns_encoding+0x25b) [0x13be6b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1273) [0x103623]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
bind(3, {sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, 12) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(bind+0xb) [0x11fcbb]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__idna_from_dns_encoding+0x2a6) [0x13beb6]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0x1273) [0x103623]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
socket(AF_INET, SOCK_DGRAM|SOCK_CLOEXEC, IPPROTO_IP) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xd53) [0x103103]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr("185.199.109.153")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa93) [0x102e43]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_UNSPEC, sa_data="\0\0\0\0\0\0\0\0\0\0\0\0\0\0"}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa4b) [0x102dfb]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr("185.199.110.153")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa93) [0x102e43]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_UNSPEC, sa_data="\0\0\0\0\0\0\0\0\0\0\0\0\0\0"}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa4b) [0x102dfb]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr("185.199.111.153")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa93) [0x102e43]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_UNSPEC, sa_data="\0\0\0\0\0\0\0\0\0\0\0\0\0\0"}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa4b) [0x102dfb]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr("185.199.108.153")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(getaddrinfo+0xa93) [0x102e43]
 > /usr/bin/wget() [0x1cdcc]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x208c8]
 > /usr/bin/wget() [0x1151e]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
socket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 3
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__socket+0xb) [0x12031b]
 > /usr/bin/wget() [0x11024]
 > /usr/bin/wget() [0x11588]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
connect(3, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr("185.199.109.153")}, 16) = 0
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__connect+0x14) [0x11fcf4]
 > /usr/bin/wget() [0x10406]
 > /usr/bin/wget() [0x3ae50]
 > /usr/bin/wget() [0x110de]
 > /usr/bin/wget() [0x11588]
 > /usr/bin/wget() [0x23425]
 > /usr/bin/wget() [0x24942]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
recvfrom(3, "HTTP/1.1 301 Moved Permanently\r\n"..., 511, MSG_PEEK, NULL, NULL) = 511
 > /usr/lib/x86_64-linux-gnu/libc.so.6(recv+0x1d) [0x11fe6d]
 > /usr/bin/wget() [0x33d72]
 > /usr/bin/wget() [0x24ccf]
 > /usr/bin/wget() [0x2e3c8]
 > /usr/bin/wget() [0x3c923]
 > /usr/bin/wget() [0xe3ff]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_init_first+0x90) [0x23a90]
 > /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x89) [0x23b49]
 > /usr/bin/wget() [0x101a5]
0 redirections exceeded.
+++ exited with 8 +++
#+end_example
