<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Words from Shane</title><link>https://shane.ai/</link><description>Recent content on Words from Shane</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 01 Sep 2023 14:42:00 -0700</lastBuildDate><atom:link href="https://shane.ai/index.xml" rel="self" type="application/rss+xml"/><item><title>CGO Performance In Go 1.21</title><link>https://shane.ai/posts/cgo-performance-in-go1.21/</link><pubDate>Fri, 01 Sep 2023 14:42:00 -0700</pubDate><guid>https://shane.ai/posts/cgo-performance-in-go1.21/</guid><description>Tl;Dr Cgo calls take about 40ns, about the same time encoding/json takes to parse a single digit integer. On my 20 core machine Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.
Disclaimer While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve maintained production applications that use Cgo and non-trivial bindings to lua.</description><content>&lt;h2 id="tl-dr">Tl;Dr&lt;/h2>
&lt;p>Cgo calls take about 40ns, about the same time &lt;code>encoding/json&lt;/code> takes to parse a single digit integer. On my 20 core machine
Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.&lt;/p>
&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve
maintained production applications that use Cgo and non-trivial bindings to lua. Performance was great. Go upgrades were a regular source of toil.
The drawback to using Cgo is losing Go cross compilation benefits and having to manage c dependencies. These days
I mainly use Cgo for compatibility and to access libraries that happen to be written in C/C++.&lt;/p>
&lt;h2 id="cgo-and-performance">Cgo &amp;amp; performance&lt;/h2>
&lt;p>Cgo performance is poorly understood in Go, and searching for information online
mixes content from 2 years ago with content from 7 years ago. Cockroach labs wrote a &lt;a href="https://www.cockroachlabs.com/blog/the-cost-and-complexity-of-cgo/">great article&lt;/a>
that measured performance and touched on the complexities of using Cgo. Since then Go performance has improved quite a but, but everything else they said is relevant.
My similar benchmarks are 17x faster than what Cockroach labs saw in 2015. Some of that might be hardware by suspect most of it is just improvements
to Go. Unfortunately I see alot of Go programmers have internalized that &amp;ldquo;Cgo is slow&amp;rdquo; without really knowing
what it&amp;rsquo;s slow compared to. Cgo is slow compared to a regular function call. It&amp;rsquo;s certainly not slow
compared to doing any sort of I/O or parsing work.&lt;/p>
&lt;p>In this post
I want to build on the idea of &amp;ldquo;&lt;a href="https://gist.github.com/jboner/2841832">latency numbers every programmer should know&lt;/a>&amp;rdquo; to figure
out where in the context of &amp;ldquo;slow&amp;rdquo; Cgo lands in the heirarchy of
L1 cache reference -&amp;gt; mutex lock -&amp;gt; main memory reference -&amp;gt; sending a packet on the network. These numbers
are from 2012 so they are really just here to give us a sense of scale:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>latency comparison numbers&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>L1 cache reference&lt;/td>
&lt;td>0.5 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Branch mispredict&lt;/td>
&lt;td>5 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>L2 cache reference&lt;/td>
&lt;td>7 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mutex lock/unlock&lt;/td>
&lt;td>25 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Main memory reference&lt;/td>
&lt;td>100 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Compress 1K bytes with Zippy&lt;/td>
&lt;td>3,000 ns 3 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Send 1K bytes over 1 Gbps network&lt;/td>
&lt;td>10,000 ns 10 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Read 4K randomly from SSD*&lt;/td>
&lt;td>150,000 ns 150 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Read 1 MB sequentially from memory&lt;/td>
&lt;td>250,000 ns 250 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Round trip within same datacenter&lt;/td>
&lt;td>500,000 ns 500 us&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>My thesis is: Cgo is has overhead, but it doesn&amp;rsquo;t have as much overhead as it used to and it
may not have as much overhead as you think.&lt;/p>
&lt;p>Lets talk about what Cgo is and a teeny bit about how it works. Cgo
is essentially Go&amp;rsquo;s ffi. When you use Cgo you can call C functions from Go and
pass information back and forth (subject to some &lt;a href="https://pkg.go.dev/cmd/cgo">rules&lt;/a>). The Go compiler autogenerates some
functions to bridge between the Go &amp;amp; C and handle
things like differences in platform calling conventions. There are also mismatches in how blocking
calls are handled and how stack is allocated that make it impractical/unsafe to run Go and C
code on the same stack. I won&amp;rsquo;t go too much into the implementation but at a high level &amp;ldquo;Cgo means IPC between threads&amp;rdquo;
is a good mental model.&lt;/p>
&lt;h2 id="benchmarking">Benchmarking&lt;/h2>
&lt;p>Lets write some benchmarks to explore performance . You can follow along at &lt;a href="https://github.com/shanemhansen/cgobench">github.com/shanemhansen/cgobench&lt;/a>. The code in the repo is autogenerated
from the &lt;a href="https://github.com/shanemhansen/shane.ai/blob/main/docs/content-org/all-posts.org">source org&lt;/a> file for this article using an implementation of Knuth&amp;rsquo;s literate programming. Is it the most productive way to write articles?
Probably not, but it&amp;rsquo;s fun and frankly playing around with new workflows helps my ADHD brain focus. But I digress.&lt;/p>
&lt;p>First off we&amp;rsquo;ll put a no-op go function in &lt;code>bench.go&lt;/code> and do a parallel benchmark. It doesn&amp;rsquo;t do anything
which is a great place to start.&lt;/p>
&lt;p>&lt;code>bench.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--Call">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">Call&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// do less
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we&amp;rsquo;ll add a simple parallel benchmark helper along with our empty call benchmark. I&amp;rsquo;m going to start out
with something so simple the compiler can inline and then compare that to a non-inlined call. When comparing
Go vs Cgo it&amp;rsquo;s important to realize that the Go compiler can&amp;rsquo;t inline Cgo functions.&lt;/p>
&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkCall">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// helper to cut down on boilerplate
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>, &lt;span style="color:#a6e22e">f&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>()) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">f&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Same as above, but explicitly calling the inlineable Call func.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkEmptyCallInlineable&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Call&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkEmptyCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span>, &lt;span style="color:#a6e22e">Call&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the case of benchmarking no-ops it&amp;rsquo;s always good to check and make sure your code didn&amp;rsquo;t get completely optimized away.
I tend to just look at the disassembled output in &lt;code>BenchmarkEmptyCall&lt;/code> and sure enough I see a convincing &lt;code>call *%rax&lt;/code> instruction in the assembly. A non dynamic
dispatch version would look like: &lt;code>call foo+0x3&lt;/code> but this version is calling a function who&amp;rsquo;s address is in the rax register.&lt;/p>
&lt;p>Let&amp;rsquo;s compile and examine:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>objdump -S cgobench.test | grep -A15 &lt;span style="color:#e6db74">&amp;#39;^0.*/cgobench.BenchmarkEmptyCall.pbench.func&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>0000000000522920 &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1&amp;gt;:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> b.RunParallel(func(pb *testing.PB) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522920: 49 3b 66 10 cmp 0x10(%r14),%rsp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522924: 76 36 jbe 52295c &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x3c&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522926: 55 push %rbp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522927: 48 89 e5 mov %rsp,%rbp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52292a: 48 83 ec 18 sub $0x18,%rsp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52292e: 48 89 44 24 10 mov %rax,0x10(%rsp)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522933: 48 8b 4a 08 mov 0x8(%rdx),%rcx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522937: 48 89 4c 24 08 mov %rcx,0x8(%rsp)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> for pb.Next() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52293c: eb 0f jmp 52294d &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x2d&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> f()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52293e: 48 8b 54 24 08 mov 0x8(%rsp),%rdx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522943: 48 8b 02 mov (%rdx),%rax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522946: ff d0 call *%rax
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we&amp;rsquo;ve verified our benchmark we can run it. I&amp;rsquo;m going to run benchmarks with a few different coure count values so we can see how the output changes. While writing this
post I experimented with some other values and for most benchmarks performance increased linearly with core count up to 16 before it began falling off.
On my machine with 20 cores the overhead of the dynamic call is around 1ns and the inlinable version is significantly faster. As
expected.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,2,4,8,16 -bench EmptyCall
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable 1000000000 0.2784 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-2 1000000000 0.1383 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-4 1000000000 0.07377 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-8 1000000000 0.04089 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-16 1000000000 0.02481 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall 718694536 1.665 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-2 1000000000 0.8346 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-4 1000000000 0.4443 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-8 1000000000 0.2385 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-16 1000000000 0.1399 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 3.819s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So now I can think of &amp;ldquo;go function call&amp;rdquo; cost as &amp;ldquo;a little more expensive than a L1 cache reference&amp;rdquo; in the above table.
What happens if we add a Cgo call?&lt;/p>
&lt;p>Below is a trivial c function to add 2 integers and a go function to call it. Note that although we might
expect gcc to inline trivial_add, we don&amp;rsquo;t expect Go&amp;rsquo;s compiler to. I did play with some even simpler
C functions but they didn&amp;rsquo;t really perform better.&lt;/p>
&lt;p>&lt;code>bench.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--cgoDefinition">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">trivial_add&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> a, &lt;span style="color:#66d9ef">int&lt;/span> b) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> a&lt;span style="color:#f92672">+&lt;/span>b;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a id="code-snippet--CallCgo">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// wow this is easy
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// import &amp;#34;C&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">CgoCall&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">C&lt;/span>.&lt;span style="color:#a6e22e">trivial_add&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkCgo">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkCgoCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span>, &lt;span style="color:#a6e22e">CgoCall&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We run benchmarks in the usual way. Single threaded Cgo overhead is about 40ns. Performance
seems to scale linearly with the number of cores up to 16ish so if I had a Cgo-bound workload I might not
bother putting it on a machine with 32 core, but real workloads usually involve more than just calling a cgo func. We can see:&lt;/p>
&lt;ul>
&lt;li>Cgo has 40ns overhead. That sits somewhere between &amp;ldquo;mutex lock&amp;rdquo; and &amp;ldquo;main memory reference&amp;rdquo;.&lt;/li>
&lt;li>40ns/op is 25 million ops/s. That&amp;rsquo;s pretty good for most projects I&amp;rsquo;ve worked on. At 4ns/op and 16 cores we&amp;rsquo;re getting 250 million ops/s.&lt;/li>
&lt;/ul>
&lt;!--listend-->
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,2,4,8,16,32 -bench Cgo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall 28711474 38.93 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-2 60680826 20.30 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-4 100000000 10.46 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-8 198091461 6.134 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-16 248427465 4.949 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-32 256506208 4.328 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 8.609s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now I want to understand a little bit more about why performance is that way. We&amp;rsquo;ll use Go&amp;rsquo;s great profiling tools to get a better picture of performance at higher core counts. I&amp;rsquo;m a fan of the pprof web view,
which tells us that &lt;code>runtime.(*timeHistorgram).record&lt;/code> and &lt;code>runtime.casgstatus&lt;/code> are taking lots of time. This
tracks with &lt;a href="https://groups.google.com/g/golang-dev/c/XSkrp1_FdiU?pli=1">Ian Lance Taylor&amp;rsquo;s observations&lt;/a>. Interestingly he doesn&amp;rsquo;t expect these operations to be contended,
so there&amp;rsquo;s potential for improving performance.&lt;/p>
&lt;p>Running the test and collecting results:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./cgobench.test -test.cpuprofile&lt;span style="color:#f92672">=&lt;/span>c.out -test.cpu&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">16&lt;/span> -test.bench Cgo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go tool pprof -png cgobench.test c.out &amp;gt; cpu.png
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-16 235322289 4.955 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note the 2 large boxes near the bottom:&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/cpu.png" style="width:50%" />
&lt;/figure>
&lt;p>I also use linux &lt;code>perf&lt;/code>. It does a good job of being able to profile cross language stuff for compiled languages as well as combining both userspace and kernel performance info.
A quick snapshot of (one of) the hot instructions in question in question from perf:&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/casgstatus.png" />
&lt;/figure>
&lt;p>Before we put it all together I&amp;rsquo;ll add one final piece of data in to help us get perspective. Here&amp;rsquo;s a carefully crafted JSON decoding benchmark that just parses an integer. It&amp;rsquo;s
written using &lt;code>json.NewDecoder&lt;/code> because just &lt;code>json.Unmarshal&lt;/code> allocates too much. What you&amp;rsquo;ll see below is that a Cgo call is 20% cheaper than a trivial JSON parse using the standard
library in both single threaded and parallel tests.&lt;/p>
&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkJSON">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkJSONCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">msg&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#e6db74">`1`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">dst&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">r&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">strings&lt;/span>.&lt;span style="color:#a6e22e">NewReader&lt;/span>(&lt;span style="color:#a6e22e">msg&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">dec&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">NewDecoder&lt;/span>(&lt;span style="color:#a6e22e">r&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">Seek&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#a6e22e">io&lt;/span>.&lt;span style="color:#a6e22e">SeekStart&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dec&lt;/span>.&lt;span style="color:#a6e22e">Decode&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">dst&lt;/span>); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,16 -bench JSON
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkJSONCall 21399691 52.79 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkJSONCall-16 217874599 5.471 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 2.942s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>So at this point we&amp;rsquo;ve measured performance overhead of Cgo, at least
in terms of wall clock time (note that we haven&amp;rsquo;t looked at memory/thread count/battery usage/etc). We know that the overhead is on the order of 2 mutex operations and that it does
scale with number of cores up to around 16. We&amp;rsquo;ve also seen that with 16 cores we can do around 4ns/op or close to 250 million Cgo ops/s. So if I was looking at using Cgo in 2023 I&amp;rsquo;d definitely use
it outside of very hot loops. There&amp;rsquo;s many reasons I wouldn&amp;rsquo;t use Cgo in 2023 (see disclaimer), but performance is unlikely to be one of them.&lt;/p>
&lt;p>I&amp;rsquo;ll end with this little Cgo version of &amp;ldquo;latency numbers every programmer should know&amp;rdquo; table:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Go/Cgo latency&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Benchmark Name&lt;/td>
&lt;td>1 core&lt;/td>
&lt;td>16 cores&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Inlined Empty func&lt;/td>
&lt;td>0.271 ns&lt;/td>
&lt;td>0.02489 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Empty func&lt;/td>
&lt;td>1.5s ns&lt;/td>
&lt;td>0.135 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>cgo&lt;/code>&lt;/td>
&lt;td>&lt;code>40 ns&lt;/code>&lt;/td>
&lt;td>&lt;code>4.281 ns&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>encoding/json int parse&lt;/td>
&lt;td>52.89 ns&lt;/td>
&lt;td>5.518 ns&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></content></item><item><title>Threads and Goroutines</title><link>https://shane.ai/posts/threads-and-goroutines/</link><pubDate>Mon, 12 Jun 2023 15:33:25 -0700</pubDate><guid>https://shane.ai/posts/threads-and-goroutines/</guid><description>So after several years of reading oversimplified and flat out incorrect comments about threads and fibers/goroutines/async/etc and fighting this reaction:
I&amp;rsquo;ve decided to write my own still-over-simplified all in one guide to the difference between a couple popular threads and fiber implementations. In order to keep this a blog post and not a novel I&amp;rsquo;m just going to focus on linux threads, go goroutines, and rust threads.
tl;dr - Rust threads on linux use 8kb of memory, Goroutines use 2kb.</description><content>&lt;p>So after several years of reading oversimplified and flat out incorrect comments about threads and fibers/goroutines/async/etc and fighting this reaction:&lt;/p>
&lt;p>&lt;img src="https://imgs.xkcd.com/comics/duty_calls.png" alt="Someone on the internet is wrong">&lt;/p>
&lt;p>I&amp;rsquo;ve decided to write my own still-over-simplified all in one guide to the difference between a couple popular threads and fiber implementations. In order to keep this a blog post
and not a novel I&amp;rsquo;m just going to focus on linux threads, go goroutines, and rust threads.&lt;/p>
&lt;p>tl;dr - Rust threads on linux use 8kb of memory, Goroutines use 2kb. It&amp;rsquo;s a big difference but nowhere near as big as the &amp;ldquo;kilobytes vs megabytes&amp;rdquo; claim I often see repeated. The magic of goroutines, if it exists, is
tied up in how those tasks are integration with non-blocking I/O in a userspace scheduler.&lt;/p>
&lt;p>I&amp;rsquo;d like to give you better tools to reason about systems engineering questions like &amp;ldquo;should we use one thread per client?&amp;rdquo; &amp;ldquo;do we need to be async to scale?&amp;rdquo; &amp;ldquo;what concurrency architecture should I choose for my next project?&amp;rdquo;&lt;/p>
&lt;p>Let&amp;rsquo;s start with an example because the rest of the article will essentially discuss these results. We&amp;rsquo;ll talk about whether or not they are surprising and the tradeoffs necessary to get them. So the first question is:&lt;/p>
&lt;h2 id="how-heavy-are-threads">How heavy are threads?&lt;/h2>
&lt;p>I first want to look at how much memory a thread uses. You can find this out simply enough on linux via &lt;code>ulimit&lt;/code> It&amp;rsquo;s changeable. Run this on your favorite linux machine to see what you get. As you can see I get 8 megabytes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ ulimit -a | grep stack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>stack size &lt;span style="color:#f92672">(&lt;/span>kbytes, -s&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#ae81ff">8192&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now this number is correct, but it&amp;rsquo;s often misinterpreted. Now you might think that to make a new thread I need 8 megabytes of RAM free. But thanks to the magic of virtual memory and overcommit, that&amp;rsquo;s not necessarily the case.
The right way to think about this is that the OS, let&amp;rsquo;s assume 64bit, is going to allocate you your own private range but this doesn&amp;rsquo;t really have to be backed by anything. There are alot of 8mb blocks in a 64bit address space.
However there is some book keeping overhead as the kernel tracks it&amp;rsquo;s IOUs.&lt;/p>
&lt;p>Let&amp;rsquo;s write a trivialish program in rust to allocate a million threads and measure the resident memory. But before we do that,
you might have to bump up a couple limits on your system to get the program to run. Here&amp;rsquo;s whad I had to do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>sysctl -w vm.max_map_count&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sysctl -w kernel.threads-max&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2000000000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I wrote this simple rust program to allocate a million threads that sleep for 1 second, and then wait for all of them.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-rust" data-lang="rust">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">use&lt;/span> std::thread;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">use&lt;/span> std::time::Duration;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">fn&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">let&lt;/span> count &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1_000_000&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#66d9ef">mut&lt;/span> handles &lt;span style="color:#f92672">=&lt;/span> Vec::with_capacity(count);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> _ &lt;span style="color:#66d9ef">in&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">..&lt;/span>count {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> handles.push(thread::spawn(&lt;span style="color:#f92672">||&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> thread::sleep(Duration::from_millis(&lt;span style="color:#ae81ff">1000&lt;/span>));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> handle &lt;span style="color:#66d9ef">in&lt;/span> handles {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> handle.join().unwrap();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s run it and see how it performs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cargo build --release
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/usr/bin/time ./target/release/threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6.17user 80.41system 0:38.55elapsed 224%CPU &lt;span style="color:#f92672">(&lt;/span>0avgtext+0avgdata 8500640maxresident&lt;span style="color:#f92672">)&lt;/span>k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0inputs+0outputs &lt;span style="color:#f92672">(&lt;/span>0major+2125114minor&lt;span style="color:#f92672">)&lt;/span>pagefaults 0swaps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So for those aren&amp;rsquo;t used to reading the somewhat cryptic output of /usr/bin/time, here&amp;rsquo;s how to look at it:&lt;/p>
&lt;ol>
&lt;li>6s of user time: so all the rust code creating/sleeping/etc took 6s.&lt;/li>
&lt;li>80s system time over 38s elapsed time. Which basically says we kept 2 cores busy for 38s and much of the work was in the kernel.&lt;/li>
&lt;li>8500640maxresidentk -&amp;gt; 8.5GB of RAM actually used. Divide that by a million threads and you get about 8KB overhead per thread/stack. That&amp;rsquo;s not too shabby for a &amp;ldquo;heavyweight&amp;rdquo; thread.&lt;/li>
&lt;/ol>
&lt;p>Virtual memory (as scientifically observed by watching &lt;code>top&lt;/code>) peaked at just under 2TB, so about 2MB per thread. How does that square with the 8MB value I said before? I don&amp;rsquo;t know. Maybe rust
passes some flags into &lt;code>clone()&lt;/code> to override the default.&lt;/p>
&lt;p>But there you have it: ignoring kernel structure tracking overhead, simple OS threads that don&amp;rsquo;t do much work use just 8KB of actual RAM on my system. What about Go?&lt;/p>
&lt;h2 id="goroutines-and-stuff">Goroutines and stuff&lt;/h2>
&lt;p>Quick disclaimer: dear pedants: I&amp;rsquo;m aware that a language and a particular implementation are different things and what I&amp;rsquo;m about to say doesn&amp;rsquo;t apply to gccgo. For the rest of this article
&amp;ldquo;Go&amp;rdquo; is both the Go programming language as well as the official Go toolchain.&lt;/p>
&lt;p>With that out of the way: what are goroutines, how do they differ from threads, and how does that make them better or worse?&lt;/p>
&lt;p>From a programmer point of view, a goroutine is basically a thread. It&amp;rsquo;s a function that runs concurrently (and potentially in parallel) with the rest of your program. Executing a function in
a goroutine can allow you to utilize more CPU cores. Go has a M:N threading model which means all your M goroutines are multiplexed over all your N threads (which are then multiplexed over all your CPUs by the kernel). Go defaults to NumThreads==NumCores,
even if you have a million goroutines. With threads you rely on the operating system to switch from one task to another. In Go some of that work happens in userspace. I&amp;rsquo;ll talk more about the details of the differences but first: let&amp;rsquo;s run a the
same &amp;ldquo;one million tasks sleeping for one second&amp;rdquo; test:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">package&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;time&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;sync&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">wg&lt;/span> &lt;span style="color:#a6e22e">sync&lt;/span>.&lt;span style="color:#a6e22e">WaitGroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">count&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#ae81ff">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Add&lt;/span>(&lt;span style="color:#a6e22e">count&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">i&lt;/span>&lt;span style="color:#f92672">:=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>;&lt;span style="color:#a6e22e">i&lt;/span>&amp;lt;&lt;span style="color:#a6e22e">count&lt;/span>;&lt;span style="color:#a6e22e">i&lt;/span>&lt;span style="color:#f92672">++&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">go&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">defer&lt;/span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Done&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">time&lt;/span>.&lt;span style="color:#a6e22e">Sleep&lt;/span>(&lt;span style="color:#a6e22e">time&lt;/span>.&lt;span style="color:#a6e22e">Second&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Wait&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s build it&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go build -o threads main.go
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/usr/bin/time ./threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>16.66user 0.68system 0:02.44elapsed 709%CPU &lt;span style="color:#f92672">(&lt;/span>0avgtext+0avgdata 2122296maxresident&lt;span style="color:#f92672">)&lt;/span>k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0inputs+0outputs &lt;span style="color:#f92672">(&lt;/span>0major+529900minor&lt;span style="color:#f92672">)&lt;/span>pagefaults 0swaps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So right off the bat we see:&lt;/p>
&lt;ol>
&lt;li>16s user time. That&amp;rsquo;s way more than rust. Because the rust example is just a shim making syscalls and go is performing scheduling work in userspace.&lt;/li>
&lt;li>0.68 system time. That&amp;rsquo;s low because Go asks the kernel to do less.&lt;/li>
&lt;li>2_122_296maxresident)k. 2 gigabytes of RAM resident or just 2KB/goroutine!&lt;/li>
&lt;li>My unscientific measurement of virtual memory also reported 2GB.&lt;/li>
&lt;/ol>
&lt;p>In this simple benchmark go is over 10x faster at creating a million threads that do some light scheduling. Memory usage is different, but generally same
order of magnitude. It would be a reasonable assumption to say that a non-trivial program would likely exceed 2KB stack starting size and cause it to grow (that&amp;rsquo;s a thing go can do) and so the real memory
usage of rust &amp;amp; go could converge pretty quickly.&lt;/p>
&lt;p>Putting on my SRE hat right now: if someone told me they wanted to run a service with a million goroutines I&amp;rsquo;d be a little nervous. If they told me they needed to run a service with
a million threads I&amp;rsquo;d be more nervous because of virtual memory overhead and managing sysctls. But today&amp;rsquo;s hardware is up for the challenge.&lt;/p>
&lt;p>So if that&amp;rsquo;s true what&amp;rsquo;s the value of goroutines? I like saving RAM and 2GB is smaller than 8GB, but frankly if I&amp;rsquo;m changing runtimes and languages for better performance I want closer to 10x real world
improvement to justify the cost.&lt;/p>
&lt;p>I get asked this question all the time while teaching Go classes at work. If goroutines are so cheap why can&amp;rsquo;t the kernel just make structures that cheap? If go can get away with small stacks why can&amp;rsquo;t the kernel? Go&amp;rsquo;s
task switching was initially &amp;ldquo;cooperative&amp;rdquo; (technically cooperative but managed by the runtime/compiler not the user) and now it&amp;rsquo;s &amp;ldquo;preemptive&amp;rdquo; so it seems like go has to do basically the same context
switching for goroutines that the kernel does for threads: namely saving/restoring registers.&lt;/p>
&lt;p>I&amp;rsquo;ll be honest: I don&amp;rsquo;t entirely know the answer but I have some clues. Go can get away with allocating smaller stacks because Go has always been able to grow the stack if needed. This
is a capability that is tied to the runtime. Regular programs using the rust Thread api (or clone or libc wrappers) may not have always been able to count on growable stacks. Because Go has a more tightly integrated userspace
scheduler and concurrency primitives sometimes it can context switch with less overhead. For example if one goroutine is writing to a channel and one goroutine is reading to a channel, it&amp;rsquo;s possible go can literally run
the reader and the writer on the same thread and take a fast path where the writer goroutine calls send and that triggers the current thread to switchto the reader goroutine.&lt;/p>
&lt;p>I also suspect (but have no proof) that the Go compiler may be able to be less conservative about register state it saves/restores. The linux kernel has to be potentially be ready for more hostile user code. In practice I think
there might be some ancient legacy registers/flags that the kernel has to handle that the Go compiler doesn&amp;rsquo;t.&lt;/p>
&lt;p>So far I&amp;rsquo;ve made goroutines sound pretty boring. They are like threads but use same order of magnitude of RAM. They occasionally can be scheduled smartly but I haven&amp;rsquo;t presented any evidence they can be scheduled/context switched
more efficiently than regular threads. The biggest real benefit I see is that I can use lots of goroutines without worrying as much about configuring system resources.&lt;/p>
&lt;p>So why do they exist and why are they awesome? The answer is actually simple, but first we have to talk about async I/O. The most scalable network I/O on linux is an asynchronous interface called &lt;code>epoll&lt;/code>. Another feature called &lt;code>io_uring&lt;/code> is shaping up to be the most scalable syscall mechanism on linux but it&amp;rsquo;s hoped Go can just switch to that when the time comes. Because these interfaces are async you don&amp;rsquo;t really block a thread on a &lt;code>.Read()&lt;/code> call. Typically we call these systems event driven and utilize &lt;code>callbacks&lt;/code>, short handler functions, to react to new data being read. Node.js for example uses libuv under the hood to do efficient non-blocking evented I/O. Go also transparently uses non-blocking I/O everywhere and it integrates that I/O scheduling with goroutines. non-blocking I/O where possible plus integration of the event loop into the go scheduler is, to answer our earlier question, the manner in which goroutines can be more efficient than threads and it&amp;rsquo;s how go manages to be pretty good at fast networking. It&amp;rsquo;s possible for a call to .Read() to submit a non-blocking I/O request and then cooperatively switch to the next goroutine much like a &lt;code>async&lt;/code> Rust function but without having the colored function problem that often leads to library bufurication. Javascript avoids this by essentially making everything async and non-blocking. Python &amp;amp; Rust have to juggle async/non-async separately.&lt;/p>
&lt;p>So when you combine goroutines and fully integrated non-blocking I/O that&amp;rsquo;s when you get strong multicore performance and a platform that can &amp;ldquo;cheaply&amp;rdquo; handle a large number of network connections while still avoiding &amp;ldquo;callback hell&amp;rdquo; or the &amp;ldquo;function coloring&amp;rdquo; problem. It&amp;rsquo;s not everyone&amp;rsquo;s desired tradeoff. They&amp;rsquo;ve made C interop more expensive, and calls that can&amp;rsquo;t be made non-blocking have to be done in a threadpool (just like node.js and DNS resolution). But if you want to productively write
some fast network servers, it&amp;rsquo;s a powerful batteries included platform.&lt;/p></content></item><item><title>About Me</title><link>https://shane.ai/about/</link><pubDate>Wed, 28 Dec 2022 14:35:59 -0800</pubDate><guid>https://shane.ai/about/</guid><description>About Me Hey, I&amp;rsquo;m Shane Hansen. You can find me online:
Linkedin: shanemhansen Github: @shanemhansen Email: shanemhansen@gmail.com I mostly write about what we call cloud computing these days and what used to be called systems engineering because as we all know:</description><content>&lt;h1 id="about-me">About Me&lt;/h1>
&lt;p>Hey, I&amp;rsquo;m Shane Hansen. You can find me online:&lt;/p>
&lt;ul>
&lt;li>Linkedin: &lt;a href="https://www.linkedin.com/in/shanemhansen/">shanemhansen&lt;/a>&lt;/li>
&lt;li>Github: &lt;a href="https://github.com/shanemhansen">@shanemhansen&lt;/a>&lt;/li>
&lt;li>Email: &lt;a href="mailto:shanemhansen@gmail.com">&lt;a href="mailto:shanemhansen@gmail.com">shanemhansen@gmail.com&lt;/a>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I mostly write about what we call cloud computing these days and what
used to be called systems engineering because as we all know:&lt;/p>
&lt;img src="cloud.jpeg" alt="Comic with son asking father what clouds are made of and father responding &amp;#39;linux servers, mostly&amp;#39;" class="center" style="border-radius: 8px;" /></content></item><item><title>Load Testing Tips</title><link>https://shane.ai/posts/load-testing-tips/</link><pubDate>Wed, 28 Dec 2022 14:35:59 -0800</pubDate><guid>https://shane.ai/posts/load-testing-tips/</guid><description>Load testing tips Over a decade plus of getting retailers ready for a smooth Black Friday I&amp;rsquo;ve collected a few tips, tricks, and stories related to keeping busy applications online during big events.
In fact there&amp;rsquo;s one simple (not easy!) trick to it: the best way to ensure your website can handle a big event is to have your website handle a big event. That may seem like a tautology, but it&amp;rsquo;s where this post starts and it&amp;rsquo;s where it ends.</description><content>&lt;h1 id="load-testing-tips">Load testing tips&lt;/h1>
&lt;p>Over a decade plus of getting retailers ready for a smooth Black Friday I&amp;rsquo;ve collected a few tips, tricks, and stories
related to keeping busy applications online during big events.&lt;/p>
&lt;p>In fact there&amp;rsquo;s one simple (not easy!) trick to it: the best way to ensure your website can handle a big
event is to have your website handle a big event. That may seem like a tautology, but it&amp;rsquo;s where this post
starts and it&amp;rsquo;s where it ends.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Why should you care what I have to say on load testing? I&amp;rsquo;ve spent a decade so far doing this for
everyone from Walmart to Google and that&amp;rsquo;s left me with a bunch of fun war stories I can package up as &amp;ldquo;best practices&amp;rdquo;. Odds
are some of the edge cases I&amp;rsquo;ve ran into will be something you&amp;rsquo;ll run into too and maybe if we&amp;rsquo;re lucky something here will help you avoid an outage.&lt;/p>
&lt;h2 id="how-not-to-load-test">How &lt;em>not&lt;/em> to load test&lt;/h2>
&lt;p>Load testing seems simple enough, but it&amp;rsquo;s a fractal of emergent complexity. To paraphrase the old saying about
backups: &amp;ldquo;customers don&amp;rsquo;t care about load tests, they care about the application working when they need it&amp;rdquo;. It&amp;rsquo;s surprisingly
easy to create load tests that give results that bear no relation to the user experience. I should know because
I&amp;rsquo;ve written my share of them.&lt;/p>
&lt;p>Here&amp;rsquo;s an example of a basic load test. Despite all the flaws we&amp;rsquo;ll discuss, I often start here due to ease of use.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ wrk -t1 -c &lt;span style="color:#ae81ff">10&lt;/span> -d10 &lt;span style="color:#e6db74">&amp;#39;https://example.com/&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Running 10s test @ https://example.com/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">1&lt;/span> threads and &lt;span style="color:#ae81ff">10&lt;/span> connections
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Thread Stats Avg Stdev Max +/- Stdev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Latency 25.00ms 13.04ms 230.48ms 92.26%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Req/Sec 135.68 49.04 202.00 78.79%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">1341&lt;/span> requests in 10.00s, 644.14KB read
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Non-2xx or 3xx responses: &lt;span style="color:#ae81ff">1341&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Requests/sec: 134.08
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Transfer/sec: 64.41KB
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://github.com/wg/wrk">wrk&lt;/a> is a reasonably good tool for generating HTTP request load. The above code uses one
thread and 10 connections to make requests for 10 seconds. I&amp;rsquo;m going to refer to this example a couple times to show how small
changes to your load testing agent can have a big impact on your results.&lt;/p>
&lt;h3 id="caching">Caching&lt;/h3>
&lt;p>If your application has caching and your load test just hammers one single URL repeatedly, it&amp;rsquo;s very likely you could get
artificially high cache hit rates and low latency. For an ecommerce application there&amp;rsquo;s often a very long tail of low traffic
product page requests that don&amp;rsquo;t have a very high cache hit rate. It&amp;rsquo;s not unusual to have a 10x difference in performance
for cached vs uncached responses. The above &lt;code>wrk&lt;/code> test is a perfect example of that.&lt;/p>
&lt;p>There are various ways to fix this, but they depend on the application and CDN configuration. It&amp;rsquo;s possible to
add cache-control headers to request the server disable a cache (in fact when you Ctrl-F5 refresh this is what your browser does). It&amp;rsquo;s also possible to add cache busting query strings via timestamp. But it&amp;rsquo;s up to you whether you want to test the
cached path, the uncached path, or as is most common: the cached path for your expected cache hit ratio and traffic distribution.&lt;/p>
&lt;h3 id="compression">Compression&lt;/h3>
&lt;p>Doing compression wrong is another very frequent mistake. Most customers who interact with your web app using a browser support gzip compression (some of them support brotli). Overall for text payloads like HTML/JSON/JS/CSS gzip compression provides huge bandwidth savings. In addition many CDNs either store responses gzipped to save space, or they store gzipped and non-gzipped responses separately (the relevant part of the HTTP spec involves the Vary header &lt;code>Vary: Accept-Encoding&lt;/code>). Compression can throw off your test results in 2 ways.&lt;/p>
&lt;ol>
&lt;li>You might be bandwidth limited during your tests if you forget to use a client that supports compression.&lt;/li>
&lt;li>I&amp;rsquo;ve seen people get poor performance when using clients that don&amp;rsquo;t support compression with a CDN that stores a single canonical gzip compressed copy of content, resulting in constant unzipping on the fly.&lt;/li>
&lt;/ol>
&lt;p>Avoiding this is generally as simple as ensuring your client sends the &lt;code>Accept-Encoding: gzip&lt;/code> header. For more advanced tests you might want to simulate multiple clients so that you can test gzip and brotli and ensure that cache forking on the 2 methods isn&amp;rsquo;t reducing your overall cache hit rate.&lt;/p>
&lt;p>During one large load test at Walmart we were both saturating bandwidth in one area and also seeing unexpectedly high CPU utilization on the cache servers. Because they stored cached text content normalized using gzip. It turned out the load agents were not configured to indicate gzip support. Which caused the cache servers to spend lots of CPU unzipping as well as waste lots of bandwidth. What at first seemed like a successful load test (at least from the pov of the team trying to find system limits) was in fact testing the wrong thing.&lt;/p>
&lt;h3 id="connection-management">Connection management&lt;/h3>
&lt;p>Connection management is &amp;ldquo;how requests get mapped onto underlying transports&amp;rdquo;. Are you using HTTP/1.1? HTTP/2? HTTP/3? With or without TLS? Keepalive? The answer affects how much load you&amp;rsquo;ll be able to generate on the applications vs the infrastructure between your client and the app. Here&amp;rsquo;s why those questions matter:&lt;/p>
&lt;h4 id="http-protocol-versions">HTTP Protocol versions&lt;/h4>
&lt;p>In the early days of HTTP servers closed the TCP connection to indicate a response was complete. One connection served one request. The problem was that TCP connections can be expensive to setup (See: &lt;a href="https://developer.mozilla.org/en-US/docs/Glossary/TCP_handshake">TCP Handshake&lt;/a> ). TLS &amp;lt; 1.2 handshakes even more so. To be honest I&amp;rsquo;m not 100% sure about TLS1.3. It gets weird with early data and I&amp;rsquo;m not going to try to go into that because I don&amp;rsquo;t understand it yet.&lt;/p>
&lt;p>Anyways developers quickly came up with a way to reuse connections by sending the &lt;code>Content-Length&lt;/code> response header which allowed a client to know when a response was done. Then a new request could be sent on the connection if both the server and client had sent a &lt;a href="https://en.wikipedia.org/wiki/Keepalive">keep-alive&lt;/a> header. When combined with connection pooling this allowed clients to have several concurrent requests multiplexed onto several TCP connections. This was standardized in HTTP/1.1, along with some fixes for streaming content.&lt;/p>
&lt;p>Until recently HTTP/1.1 was the protocol used by default when a CDN connected to your website origin, although as of writing this blog I see that both Google &lt;a href="https://cloud.google.com/media-cdn/docs/origins">Media CDN&lt;/a> and &lt;a href="https://developers.cloudflare.com/cache/how-to/enable-http2-to-origin/">Cloudflare CDN&lt;/a> support HTTP/2 to origin as well although it appears some other popular CDNs such as Akamai &lt;a href="https://myakamai.force.com/customers/s/question/0D54R00007GkHvvSAF/does-akamai-support-http2-between-edge-to-origin?language=en_US">do not&lt;/a>. HTTP/2 addresses some of the problems of HTTP/1.1 by allowing multiple request/responses to be sent concurrently over the same connection. Although you could send multiple requests with HTTP/1.1 you generally had to wait for a response before you sent a new request (ignoring &lt;a href="https://en.wikipedia.org/wiki/HTTP_pipelining">pipelining&lt;/a> which is rarely used). This leads to a problem called &lt;a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">head of line blocking&lt;/a> wherein delays in processing a single request will hold up all the others. The biggest change in HTTP/2 is that multiple connections are no longer needed and it generally runs over TLS (The TCP only version of HTTP/2 is not as well supported). However it can still suffer from head of line blocking because TCP delivers packets in order so one dropped packet means everything on the multiplexed connection stalls. There&amp;rsquo;s also HTTP/3 which is built on top of UDP which enables &lt;a href="https://calendar.perfplanet.com/2020/head-of-line-blocking-in-quic-and-http-3-the-details/#sec_http3">HTTP/3 to solve&lt;/a> the head of line blocking problem.&lt;/p>
&lt;h4 id="connection-configuration-tips">Connection configuration tips&lt;/h4>
&lt;p>Here&amp;rsquo;s what this means for performance testing:&lt;/p>
&lt;p>If you don&amp;rsquo;t use keepalive you&amp;rsquo;ll create tons of TCP connections. That&amp;rsquo;s good if you want to test your load balancer and find out how well you&amp;rsquo;ve tuned your TCP stack on the load generating machines. In all likelihood you will quickly exhaust your ephemeral ports if you do a naive test w/o keepalive. Personally I&amp;rsquo;d likely use HTTP/1.1 over TLS with keepalive for most application load testing. Ideally you want to use what your customers use which is usually HTTP/2, but HTTP/2 support in load testing tools can be spotty, and depending on your CDN/Load Balancer setup odds are you&amp;rsquo;re speaking HTTP/1.1 to origin anyways.&lt;/p>
&lt;p>You can increase that port range on linux systems as well as allow the kernel to more quickly recycle ports with the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#ae81ff">1024&lt;/span> &lt;span style="color:#ae81ff">65535&lt;/span> &amp;gt; /proc/sys/net/ipv4/ip_local_port_range
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sysctl -w net.ipv4.tcp_tw_reuse &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here&amp;rsquo;s a skeleton of a &lt;code>wrk&lt;/code> command that does keepalive and compression over HTTP/1.1 correctly.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># wrk defaults to keepalive&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>wrk -c $CONNECTION_COUNT -t $THREADS -d $DURATION &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -H &lt;span style="color:#e6db74">&amp;#39;Accept-Encoding: gzip&amp;#39;&lt;/span> &lt;span style="color:#e6db74">&amp;#39;https://website/&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Like everything else in engineering, there are tradeoffs. The single best load test you could do would be to have agents everywhere your customers are making connections and requests exactly like your customers. There are some ways to do that, but just as normal software testing involves running fast unit tests more frequently and slower integration tests less often, a good load testing strategy often involves frequent tests with something like &lt;code>wrk&lt;/code> and less frequent high fidelity tests across multiple regions. Most of the time I want to use just a few connections to hammer the application, but occasionally I want to make sure the TCP side of the stack is up to snuff.&lt;/p>
&lt;h3 id="unrepresentative-client-locations">Unrepresentative client locations&lt;/h3>
&lt;p>This is a really broad one, but what happens is that generally your customers are all over your country or possibly all over the world. Quite often a load test is being run from a single region. This can distort results in all sorts of fun ways such as:&lt;/p>
&lt;h4 id="cdn-pop-overload">CDN Pop Overload&lt;/h4>
&lt;p>Your CDN likely routes customers to the closest PoP (Point of Presence). Your load test could overload a single PoP and leave the rest of the CDN and your datacenters with no traffic. Many engineers accidentally load test CDN hot spot mitigation paths, not their app.&lt;/p>
&lt;h4 id="unbalanced-network-traffic">Unbalanced network traffic&lt;/h4>
&lt;p>It is very possible that if all load is coming from a single region, parts along the way can get overloaded. As an example many load balancers doing some form of &lt;a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP&lt;/a> will direct traffic based on some hash of connection information which can result in hot spots if there aren&amp;rsquo;t enough connections. Similarly intermediate routers can be overloaded. For that reason I recommend using multiple load generating agents.&lt;/p>
&lt;p>From a practical perspective I like to use something like &lt;a href="https://cloud.google.com/run">Cloud Run&lt;/a> because it scales to 0 based on traffic so it&amp;rsquo;s pretty cheap to deploy a container to every single region.&lt;/p>
&lt;p>At one point Walmart load tests seemed to hit a ceiling. We couldn&amp;rsquo;t push load any higher, but paradoxically none of the systems handling the load seemed to be at saturation. We ruled out CPU/RAM and bandwidth to individual machines. We went back and checked our work to verify multi-core scalability and came up with nothing. It was unfortunately a stressful time with many teams grasping at straws to figure out the culprit. One of the most interesting clues was that the ceiling happened at a suspiciously round number. Let&amp;rsquo;s say 4Gbps. After a lot of investigation that probably deserves it&amp;rsquo;s own post I realized that we had 2 circuits coming into our DC (lest you think the fact this happened on-prem means you don&amp;rsquo;t have to worry: customers using the Cloud at large scales have lots of analogous things like interconnects and gateways). Theoretically we should have had 2 20Gbps links and they shouldn&amp;rsquo;t have been saturated. After picking up a phone and calling the guy who was responsible for purchasing those I found out from him that: &amp;ldquo;we just brought the new one online, I wonder if the ISP left it at 2Gbps&amp;rdquo;. Which it turns out was the case. One phone call later and we were back in business. Some fraction of our TCP flows from our load testing agents were going to through the under provisioned link and that was creating artificial back pressure on the load testing agents. It just goes to show the importance of monitoring on all your dependencies physical or virtual. I&amp;rsquo;ve seen the same sort of issues with naively written cloud NAT &amp;ldquo;appliances&amp;rdquo; (linux vms) that lack monitoring of &lt;code>nf_conntrack&lt;/code> table size, resulting in mostly-silent degradation.&lt;/p>
&lt;p>This has informed my performance monitoring philosophy which is an extension of my unit testing philosophy.&lt;/p>
&lt;blockquote>
&lt;p>If there&amp;rsquo;s not a test proving it works, it doesn&amp;rsquo;t.&lt;/p>
&lt;/blockquote>
&lt;p>and&lt;/p>
&lt;blockquote>
&lt;p>If you&amp;rsquo;re not monitoring performance, it&amp;rsquo;s degraded.&lt;/p>
&lt;/blockquote>
&lt;h3 id="unrealistic-traffic">Unrealistic traffic&lt;/h3>
&lt;p>Here&amp;rsquo;s a very common story at large retailers. Someone runs a load test. The app passes with flying colors. Organic traffic begins to ramp up. The website crashes. What happened?&lt;/p>
&lt;p>Well it turns out naive load tests often fail to exercise critical components. Here&amp;rsquo;s just a handful of ways I&amp;rsquo;ve seen this happen:&lt;/p>
&lt;p>The load test was detected as a bot and all those super fast 200 responses were captcha pages. Nobody bothered to verify that the page under test was returning the correct content. I wish I could tell a war story here, but there&amp;rsquo;s probably too many to choose from.&lt;/p>
&lt;blockquote>
&lt;p>If you&amp;rsquo;re not asserting you&amp;rsquo;re getting the expected content, you&amp;rsquo;re not.&lt;/p>
&lt;/blockquote>
&lt;p>The load test pulled down a product page (eg &lt;code>https://example.com/product/123&lt;/code>) but since this is a SPA (Single Page App) all the important API calls happened via &lt;code>fetch&lt;/code>/&lt;code>XHR&lt;/code> and so all the load test really did was pull down an empty shell of a page.&lt;/p>
&lt;p>The load test pulled down all the HTML, but none of the associated resources such as images/css/javascript etc. More than once in my career I&amp;rsquo;ve seen developers put timestamp cache busters on their static content, only to cause an outage when they deploy and every single request from customers is a cache miss (eg &lt;code>https://example.com/static/image.jpg?_=$(date +%s)&lt;/code>).&lt;/p>
&lt;h2 id="how-to-create-load-tests-that-give-you-confidence-your-web-app-will-scale">How to create load tests that give you confidence your web app will scale&lt;/h2>
&lt;p>What you want is to simulate a bunch of real customers doing a bunch of realistic things on your web app. Real customers come from different locations. They use different browsers. They look at different products. They do things like search/login/add to chart/checkout/etc. They execute javascript and download images. They use browsers that &lt;a href="https://caniuse.com/http2">support HTTP/2&lt;/a> or &lt;a href="https://caniuse.com/http3">HTTP/3&lt;/a>. Of course running a load test with chrome is a lot more resource intensive than running a load test with &lt;code>wrk&lt;/code>, just like an end to end integration test is more expensive than a unit test.&lt;/p>
&lt;p>I recommend you start out with benchmarks alongside your repo&amp;rsquo;s unit tests. These should be ran frequently and tracked in your metrics system. For every release you should know how the behavior of core endpoints like &lt;code>GET /resource/foo&lt;/code> behaves under load, possibly with mocked data store or API dependencies. Problems found here are cheapest to fix. In Go this looks like something using the &lt;a href="https://pkg.go.dev/net/http/httptest">httptest&lt;/a> package and their &lt;a href="https://pkg.go.dev/testing#hdr-Benchmarks">Benchmark&lt;/a> package. Now it&amp;rsquo;s often the case that the team writing load tests is not the app development team, but SRE best practices show that cross functional teams co-developing features results in fewer big issues during launch. A trivial-ish example might look like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkApp&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">handler&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">MyNewApplicationHandler&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">req&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">httptest&lt;/span>.&lt;span style="color:#a6e22e">NewRequest&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;GET&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;http://example.com/foo&amp;#34;&lt;/span>, &lt;span style="color:#66d9ef">nil&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">w&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">httptest&lt;/span>.&lt;span style="color:#a6e22e">NewRecorder&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">handler&lt;/span>(&lt;span style="color:#a6e22e">w&lt;/span>, &lt;span style="color:#a6e22e">req&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// validate response recorder `w` returns 200 and correct content
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next layer up is &amp;ldquo;simple&amp;rdquo; scripted CLI load tests. I like &lt;code>wrk&lt;/code> with lua scripting which can do a pretty good job of hitting a bunch of random URLs quickly and defeating caching if needed. It doesn&amp;rsquo;t support http/2 but &lt;a href="https://github.com/tsliwowicz/go-wrk">go-wrk&lt;/a> does. Properly configured with compression support this is a great workhorse for individual teams to test their critical endpoints. You can also do more complicated tests such as requesting a resource and then dependent resources but that can take a lot of work and knowledge of the application to capture the right requests to make. People often use Siege here or even Apache Jmeter.&lt;/p>
&lt;p>Here&amp;rsquo;s an example with &lt;code>wrk&lt;/code> (adapted from &lt;a href="https://medium.com/@felipedutratine/intelligent-benchmark-with-wrk-163986c1587f">Intelligent benchmark with wrk&lt;/a> ) which randomly selects between 1000 product URLs. Lua scripting is very powerful here for creating custom benchmark scenarios.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-lua" data-lang="lua">&lt;span style="display:flex;">&lt;span>math.randomseed(os.time())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>request &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">function&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url_path &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/product/&amp;#34;&lt;/span> &lt;span style="color:#f92672">..&lt;/span> math.random(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> wrk.format(&lt;span style="color:#e6db74">&amp;#34;GET&amp;#34;&lt;/span>, url_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You would use this script like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>wrk -c10 -t1 -d10s -H &lt;span style="color:#e6db74">&amp;#39;Accept-Encoding: gzip&amp;#39;&lt;/span> -s ./wrk.lua
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally for full scale high fidelity load tests there are relatively few tools out there for browser based load testing. Unfortunately the automation frameworks out there such as selenium can be fragile and have lots of overhead and so &lt;a href="https://www.selenium.dev/documentation/test_practices/discouraged/performance_testing/">they discourage using their libraries for load testing&lt;/a>.&lt;/p>
&lt;p>There are essentially three approaches I like to use.&lt;/p>
&lt;p>The first one feels like cheating but after several years preparing Walmart for Black Friday I can attest to it&amp;rsquo;s unreasonable effectiveness. Before your big event (such as Black Friday) hold some sort of flash sale/small event. For example one year there was a highly anticipated low-quantity device that had just been released. My company stocked a few on their website and sent out an email marketing blast and instantly generated the best load test money can buy as customers flocked to the website to get a good deal. We of course had a minor outage and then had time to optimize and fix before Black Friday/Cyber Monday. If you can run some sort of test marketing event before the big event you absolutely should.&lt;/p>
&lt;p>On the more technical side: it&amp;rsquo;s possible to record a real customer interaction and export HAR (http archive) files. Those can be &lt;a href="https://www.flood.io/blog/convert-har-files-to-jmeter-test-plans">imported into something like jmeter&lt;/a>, or you can write your own converter. I haven&amp;rsquo;t tried this approach yet.&lt;/p>
&lt;p>My preferred approach at the moment is to use headless chrome on cloud run to generate load from multiple regions. With a little scripting in &lt;a href="https://chromedriver.chromium.org/home">chromedriver&lt;/a> it&amp;rsquo;s possible to load a bunch of pages, take screenshots, and export all the timing metrics for later processing. I use &lt;a href="https://cloud.google.com/workflows">Cloud Workflows&lt;/a> to orchestrate the process and take care of ramping up traffic and collecting summary statistics. I&amp;rsquo;m working on open sourcing my workflows/containers here to allow others to easily spin up real-browser load tests. Stay tuned.&lt;/p></content></item></channel></rss>