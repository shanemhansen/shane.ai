<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Words from Shane</title><link>https://shane.ai/posts/</link><description>Recent content in Posts on Words from Shane</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 26 Sep 2023 13:34:00 -0700</lastBuildDate><atom:link href="https://shane.ai/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Some notes on 'javascript jit and deopt'</title><link>https://shane.ai/posts/notes-on-javascript-jit-deopt/</link><pubDate>Tue, 26 Sep 2023 13:34:00 -0700</pubDate><guid>https://shane.ai/posts/notes-on-javascript-jit-deopt/</guid><description>Recently I read a fantastic article walking through jit optimizations and how changes to source code could impact those: Side effecting a deopt.
As I shared it with folks, a few of them had some questions about low level optmizations in general and I wrote this as a little explainer for people who are interested in learning more about how javascript runtimes can model/compile/jit/execute their js code. So I wrote this explainer to go along with the original article.</description><content>&lt;p>Recently I read a fantastic article walking through jit optimizations
and how changes to source code could impact those: &lt;a href="https://www.recompiled.dev/blog/deopt/">Side effecting a deopt&lt;/a>.&lt;/p>
&lt;p>As I shared it with folks, a few of them had some questions about low level optmizations in general and
I wrote this as a little explainer for people who are interested in learning more about how
javascript runtimes can model/compile/jit/execute their js code. So I wrote this explainer
to go along with the original article. Important: please read the original article first or have it pulled up next to this
article.&lt;/p>
&lt;p>My goal is that by the time we&amp;rsquo;re done the reader understands a bit more about:&lt;/p>
&lt;ol>
&lt;li>How their computer can model arbitrary property/value pairs (&lt;code>Objects&lt;/code>)&lt;/li>
&lt;li>How their computer can model &lt;code>Objects&lt;/code> with fixed properties (what many languages would call a struct, or even a class)&lt;/li>
&lt;li>Some basics about how a javascript engines can observe how a value runs through the system&lt;/li>
&lt;/ol>
&lt;h2 id="understanding-hexadecimal-notation-and-ram">Understanding hexadecimal notation and RAM&lt;/h2>
&lt;p>Many programs that deal with memory addresses use hexadecimal. So instead of saying &amp;ldquo;20th byte&amp;rdquo; they say &amp;ldquo;0x14&amp;rdquo;. Base 16 just adds 6 extra &amp;ldquo;digits&amp;rdquo; and uses
a-f to represent them. So here&amp;rsquo;s a few numbers in hex and base 10. We often write a prefix &lt;code>0x&lt;/code> for hex numbers to let you know they aren&amp;rsquo;t base 10.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>base 10&lt;/th>
&lt;th>hex&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>a&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>b&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>14&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For those who like a more theoretical description: the idea is that any number we work with like &lt;code>123&lt;/code> really means: &lt;code>1*10^2+2*20^1+3*10^0&lt;/code>. If you look at our 0x14 example that means:
&lt;code>0x14 = 1*16^1+4*16^0 == 20&lt;/code>.&lt;/p>
&lt;p>About the only time I mention hex is when discussing the output of optimization tools and it&amp;rsquo;s really simple
because we&amp;rsquo;ll deal with things like &lt;code>0x2&lt;/code> which is 2. And &lt;code>0xc&lt;/code> which is 13.&lt;/p>
&lt;h2 id="disclaimers">Disclaimers:&lt;/h2>
&lt;p>I&amp;rsquo;m going to be using the madeup phrase &amp;ldquo;RAM indexes&amp;rdquo;. The real world calls &amp;ldquo;RAM indexes&amp;rdquo; pointers. I&amp;rsquo;m
hopeful that using &amp;ldquo;RAM index&amp;rdquo; as if RAM is an array of bytes is clearer for the target audience. But feel free
to translate statements like &lt;code>RAM[foo]&lt;/code> to &lt;code>*foo&lt;/code>. Similarly I will talk alot about records that are packed,
the normal industry nomenclature would probably be &lt;code>C struct&lt;/code> (with some caveats around packing, padding, field ordering, etc).
I&amp;rsquo;m pretending in this example that our computers have byte sized words and that ASCII is great because
32bit/64bit and utf8/utf16 don&amp;rsquo;t add anything to this post and we&amp;rsquo;d have to count in multiples of 4 or 8.
Finally my hashtables aren&amp;rsquo;t fast at all. I literally just want people to conceptualize the basic idea of hashing a key to find a bucket
as an alternative to some sort of linear search.&lt;/p>
&lt;h2 id="how-to-represent-data-in-ram">How to represent data in RAM&lt;/h2>
&lt;p>The atomic unit of data we&amp;rsquo;ll talk about today is a byte. It&amp;rsquo;s a series of 8 ones and zeros. Or it&amp;rsquo;s a number between 0 and 255 (because &lt;code>2^8=256&lt;/code>).&lt;/p>
&lt;p>If we want to model some sort of record on the computer, such as a person who has a numeric id and a numeric age, we have to come up with a way of
representing those objects in memory and referring to them. Most of the readers here understand what I mean when I say something like &lt;code>let Person = {id:0, age:24}&lt;/code>, but computers don&amp;rsquo;t.
The simplest way to represent a person &amp;ldquo;object&amp;rdquo; then is as 2 bytes next to each other. The first one is their id. The second one is their age. Let&amp;rsquo;s write out
an array of 2 persons in RAM:&lt;/p>
&lt;p>&lt;code>let persons = [{id:1, age:24}, {id:2, age:28}]&lt;/code>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>RAM index&lt;/th>
&lt;th>0&lt;/th>
&lt;th>1&lt;/th>
&lt;th>2&lt;/th>
&lt;th>3&lt;/th>
&lt;th>4&lt;/th>
&lt;th>5&lt;/th>
&lt;th>6&lt;/th>
&lt;th>7&lt;/th>
&lt;th>8&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RAM value&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24&lt;/td>
&lt;td>2&lt;/td>
&lt;td>28&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Field name&lt;/td>
&lt;td>id&lt;/td>
&lt;td>age&lt;/td>
&lt;td>id&lt;/td>
&lt;td>age&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So now if you tell a computer where the object starts (via pointer or RAM index), it knows the &lt;code>id&lt;/code> is at offset 0, and the &lt;code>age&lt;/code> is at offset 1. It knows the size of a &lt;code>person&lt;/code> record (2 bytes).
If it needs to operate on those values (LOAD/STORE/MOV low level assembly instructions)
it can directly write machine code that uses those offsets. Let&amp;rsquo;s write some psuedocode for returning the age of the 2nd person that somewhat mirrors the actual instructions your computer
executes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">persons_array&lt;/span> &lt;span style="color:#f92672">=&lt;/span> ;&lt;span style="color:#75715e">// some number that is an index into RAM
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">size_of_person&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">age_offset&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">person_2_age&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">RAM&lt;/span>[
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">persons_array&lt;/span> &lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#75715e">// where the array starts
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">size_of_person&lt;/span> &lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#75715e">// skip ahead 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#a6e22e">age_offset&lt;/span> &lt;span style="color:#75715e">// age is 2nd value
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// or equivalantly: one addition. one ram lookup.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">person_2_age&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">RAM&lt;/span>[&lt;span style="color:#a6e22e">persons_array&lt;/span>&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span>];
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s show one more example for strings. Unfortunately strings are variable length. So if you have a record like &lt;code>let person = [{id:&amp;quot;id2&amp;quot;, age:29}, {id:&amp;quot;id10&amp;quot;, age:40}]&lt;/code> those
strings can&amp;rsquo;t be packed into the same orderly layout as above where each field starts at a fixed offset. We typically store the strings somewhere else and store the RAM index of that location in the person record. Let&amp;rsquo;s draw that out: because these
strings are variable length we use a special marker character called null or &lt;code>\0&lt;/code> to indicate end of string. This is how the C programming language represents strings historically.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>RAM index&lt;/th>
&lt;th>0&lt;/th>
&lt;th>1&lt;/th>
&lt;th>2&lt;/th>
&lt;th>3&lt;/th>
&lt;th>4&lt;/th>
&lt;th>5&lt;/th>
&lt;th>6&lt;/th>
&lt;th>7&lt;/th>
&lt;th>8&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RAM value&lt;/td>
&lt;td>4&lt;/td>
&lt;td>29&lt;/td>
&lt;td>8&lt;/td>
&lt;td>30&lt;/td>
&lt;td>i&lt;/td>
&lt;td>d&lt;/td>
&lt;td>2&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>i&lt;/td>
&lt;td>d&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>\0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>field name&lt;/td>
&lt;td>id&lt;/td>
&lt;td>age&lt;/td>
&lt;td>id&lt;/td>
&lt;td>age&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The big thing to notice here is the RAM value for the &lt;code>id&lt;/code>. &lt;code>4&lt;/code> and &lt;code>8&lt;/code>. Those correspond to RAM index &lt;code>4&lt;/code> and &lt;code>8&lt;/code>. Saying &amp;ldquo;here&amp;rsquo;s a fixed sized number who&amp;rsquo;s value is where you can find the variable
sized data&amp;rdquo;. So now you have enough background to understand a really basic record/object/struct type. This is an idealized example, in the real world the actual layout can be more complicated but
for our purposes today this is a good enough mental model of &amp;ldquo;how to layout an object in memory to make it easy for the CPU to get your data&amp;rdquo;.&lt;/p>
&lt;h2 id="hashtable-basics">Hashtable basics&lt;/h2>
&lt;p>Now the problem is that the above techniques do not allow us to deal with arbitrary field names. We can&amp;rsquo;t add fields later on or that would mess up all the calculations like &amp;ldquo;load the 2nd byte of the
record to get the &lt;code>age&lt;/code>&amp;rdquo;. Javascript &lt;code>Objects&lt;/code> allow any number of property/value pairs (in the context of hashtables properties are called keys) and people need to quickly look up a key when they write &lt;code>x.foo&lt;/code>. Let&amp;rsquo;s sketch a couple ways people could store those in memory.
The simplest method is to put all the values right next to each other one after another and use &lt;code>\0&lt;/code> to separate them. Here&amp;rsquo;s a way to encode &lt;code>{&amp;quot;key1&amp;quot;:&amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;:&amp;quot;value2&amp;quot;}&lt;/code>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>RAM index&lt;/th>
&lt;th>0&lt;/th>
&lt;th>1&lt;/th>
&lt;th>2&lt;/th>
&lt;th>3&lt;/th>
&lt;th>4&lt;/th>
&lt;th>5&lt;/th>
&lt;th>6&lt;/th>
&lt;th>7&lt;/th>
&lt;th>8&lt;/th>
&lt;th>9&lt;/th>
&lt;th>10&lt;/th>
&lt;th>11&lt;/th>
&lt;th>12&lt;/th>
&lt;th>13&lt;/th>
&lt;th>14&lt;/th>
&lt;th>15&lt;/th>
&lt;th>16&lt;/th>
&lt;th>17&lt;/th>
&lt;th>18&lt;/th>
&lt;th>19&lt;/th>
&lt;th>20&lt;/th>
&lt;th>21&lt;/th>
&lt;th>22&lt;/th>
&lt;th>23&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RAM value&lt;/td>
&lt;td>k&lt;/td>
&lt;td>e&lt;/td>
&lt;td>y&lt;/td>
&lt;td>1&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>v&lt;/td>
&lt;td>a&lt;/td>
&lt;td>l&lt;/td>
&lt;td>u&lt;/td>
&lt;td>e&lt;/td>
&lt;td>2&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>k&lt;/td>
&lt;td>e&lt;/td>
&lt;td>y&lt;/td>
&lt;td>2&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>v&lt;/td>
&lt;td>a&lt;/td>
&lt;td>l&lt;/td>
&lt;td>u&lt;/td>
&lt;td>e&lt;/td>
&lt;td>2&lt;/td>
&lt;td>\0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So if we stored data this way we could pretty easily write some psuedocode for looking up a key/value pair. Just compare the current key in memory with the
property name you are looking for byte by byte. If it doesn&amp;rsquo;t match, skip the next value until you find a null, and start over. Unfortunately this would be slow because
the farther along your key is, the longer you have to search. So we have something called hashtables. The basic idea of a hashtable is we have some function to convert our
keys to numbers. Then we convert those numbers to a bucket index. A hashtable has several fixed sized buckets right next to each other in RAM. So I might have 10 buckets and when I store &amp;ldquo;foo&amp;rdquo; I run a
function which says hash(&amp;ldquo;foo&amp;rdquo;) = some random number like 12313213
and then I convert that to a bucket index by saying &lt;code>12313213%10 = 3&lt;/code>. Then in bucket number 3 I store something like: &lt;code>foo\0value\0&lt;/code> (well actually it&amp;rsquo;s fixed sized. In bucket number 3 I store the RAM index of some other piece of memory that holds &lt;code>foo\0value\0&lt;/code>. The great thing about this
is that as long as there are no collisions (2 different keys that go to the same bucket) I can find any property without looking at all the data. The overhead for finding a value in a hashtable
without collisions just depends on how long it takes to run &lt;code>hash(key)&lt;/code> and not on the number of keys in the hashtable. I can handle collisions (multiple properties
ending up in the same bucket) by just using the strategy listed above and smashing key value pairs together. Without going into too much detail, real world hashtables have better strategies for minimizing collisions by detecting
when they are close to full and resizing. Sometimes instead of a long chain of key/value pairs they have another hashtable, or a tree. But for today let&amp;rsquo;s try and show an example of how a 3 key value pairs might be represented in a simple 2 bucket hashtable. The example record is: &lt;code>let person = {name: &amp;quot;bob&amp;quot;, id: &amp;quot;id3&amp;quot;, age:&amp;quot;21&amp;quot;}&lt;/code> I&amp;rsquo;m going to
draw this as a graph and then also as a linear array of bytes if we just append key/value pairs when there&amp;rsquo;s a collision.&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/ox-hugo/hashtable.png" />
&lt;/figure>
&lt;p>Here&amp;rsquo;s a table of numbers for representing the same thing. We have a person value which consists of a bucket count and a buckets RAM index. The bucket&amp;rsquo;s RAM index
is where the bucket list starts. Next up we have the 2 buckets. In the real world all of this wouldn&amp;rsquo;t be packed so close. Which is important for adding more buckets
and more key/values. Our first bucket points to index 4, which is where they key/valuy pair &amp;ldquo;name:bob&amp;rdquo; is stored, etc.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>RAM index&lt;/th>
&lt;th>0&lt;/th>
&lt;th>1&lt;/th>
&lt;th>2&lt;/th>
&lt;th>3&lt;/th>
&lt;th>4&lt;/th>
&lt;th>5&lt;/th>
&lt;th>6&lt;/th>
&lt;th>7&lt;/th>
&lt;th>8&lt;/th>
&lt;th>8&lt;/th>
&lt;th>9&lt;/th>
&lt;th>10&lt;/th>
&lt;th>11&lt;/th>
&lt;th>12&lt;/th>
&lt;th>13&lt;/th>
&lt;th>14&lt;/th>
&lt;th>15&lt;/th>
&lt;th>16&lt;/th>
&lt;th>17&lt;/th>
&lt;th>18&lt;/th>
&lt;th>19&lt;/th>
&lt;th>20&lt;/th>
&lt;th>21&lt;/th>
&lt;th>22&lt;/th>
&lt;th>23&lt;/th>
&lt;th>24&lt;/th>
&lt;th>25&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RAM value&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>17&lt;/td>
&lt;td>n&lt;/td>
&lt;td>a&lt;/td>
&lt;td>m&lt;/td>
&lt;td>e&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>b&lt;/td>
&lt;td>o&lt;/td>
&lt;td>b&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>a&lt;/td>
&lt;td>g&lt;/td>
&lt;td>e&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>i&lt;/td>
&lt;td>d&lt;/td>
&lt;td>\0&lt;/td>
&lt;td>i&lt;/td>
&lt;td>d&lt;/td>
&lt;td>3&lt;/td>
&lt;td>\0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>field&lt;/td>
&lt;td>bucketCount&lt;/td>
&lt;td>bucketRamIndex&lt;/td>
&lt;td>bucket[0]&lt;/td>
&lt;td>bucket[1]&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So this should give you a basic idea that looking up a hashtable key stays fast as you add more items, as long as you don&amp;rsquo;t have alot more items than buckets. But now instead of the
cpu being able to translate something like &lt;code>person.name&lt;/code> into a fixed offset from person, it must instead do the following psuedocode.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// imaginary ram already defined
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">RAM&lt;/span> &lt;span style="color:#f92672">=&lt;/span> [];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// RAM index where person is located.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">person&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// we&amp;#39;ve defined it to be the 1st element
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">bucketCountOffset&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">//we&amp;#39;ve deinfed it to be the 2nd element
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">bucketRamIndexOffset&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// the size of the bucket would have to be stored
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">bucketCount&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">RAM&lt;/span>[&lt;span style="color:#a6e22e">person&lt;/span>&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#a6e22e">bucketCountOffset&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// hash is defined elsewhere. It takes a string and returns an integer.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">bucketNumber&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">hash&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;person&amp;#34;&lt;/span>)&lt;span style="color:#f92672">%&lt;/span>&lt;span style="color:#a6e22e">bucketCount&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">startOfbuckets&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">RAM&lt;/span>[&lt;span style="color:#a6e22e">person&lt;/span>&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#a6e22e">bucketRamIndexOffset&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">bucketRAMIndex&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#a6e22e">RAM&lt;/span>[&lt;span style="color:#a6e22e">startOfBuckets&lt;/span>&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#a6e22e">bucketNumber&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// search_bucket searches a chain of &amp;#34;key\0value\0key\0value\0&amp;#34; pairs.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">age&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">search_bucket&lt;/span>(&lt;span style="color:#a6e22e">bucketRAMINDEX&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this requires us to look at several bytes in the property name, do some math, and load several values from RAM just to get to where age is stored. Going into the latency
of all these operations is beyond the scope here, but it&amp;rsquo;s safe to say that two things that often make code slow are:&lt;/p>
&lt;ol>
&lt;li>Searching for values without any ordering/index&lt;/li>
&lt;li>A bunch of chained RAM lookups such as the 3 RAM lookups we had to do, one of which was chained (often referred to as pointer chasing).&lt;/li>
&lt;/ol>
&lt;h2 id="optimizing-a-hashtable">Optimizing a hashtable&lt;/h2>
&lt;p>So now the optimizations javascript runtimes want to do start to make sense. Any js &lt;code>Object&lt;/code> &lt;strong>could&lt;/strong> be a full hashtable, but since so often
js Objects have fixed property names, the runtime would prefer to represent them similar to the 2 byte packed format we used above for person. In fact
it turns out that this happens pretty often. After all our example above where we store a list of objects with the same structure comes up pretty often.&lt;/p>
&lt;p>Javascript runtimes detect these patterns and optimize the storage layout and data access. It&amp;rsquo;s alot harder than you&amp;rsquo;d expect because javascript is such
a dynamic language. So not only must they make the optimizations based on assumptions like &amp;ldquo;nobody will add keys to this object&amp;rdquo;, but they must keep track
of when those assumptions are violated and fall back to regular hashtables or unoptimized code. They have to do this entire dance on a time/memory budget because
afterall they are trying to speed up your code so if their transformations and untransformations aren&amp;rsquo;t fast enough, they&amp;rsquo;ve failed at their purpose.&lt;/p>
&lt;p>So a good mental model is &amp;ldquo;javascript runtimes look at data access and speculatively optimize the storage, while maintaining a list of assumptions and dependencies that may
invalidate their assumptions. They might have to deoptimize/reoptimize if one of those assumptions changes&amp;rdquo;.&lt;/p>
&lt;h2 id="a-simple-example-to-optimize">A simple example to optimize&lt;/h2>
&lt;p>Let&amp;rsquo;s look at a javascript function from the blog:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">x&lt;/span> &lt;span style="color:#f92672">=&lt;/span> { &lt;span style="color:#a6e22e">foo&lt;/span>&lt;span style="color:#f92672">:&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> };
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">function&lt;/span> &lt;span style="color:#a6e22e">load&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#a6e22e">x&lt;/span>.&lt;span style="color:#a6e22e">foo&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#a6e22e">load&lt;/span>());
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now just by looking at this function it&amp;rsquo;s pretty easy to see that in the absence of &amp;ldquo;funny business&amp;rdquo;
(certain javascript dynamic features that can cause weird things to happen) it seems like our
entire program could easily be reduced to &lt;code>console.log(1)&lt;/code>. Let&amp;rsquo;s trace through how a js engine
might execute each version of the program.&lt;/p>
&lt;h3 id="execution-trace">Execution trace&lt;/h3>
&lt;ol>
&lt;li>Create a new string value &lt;code>&amp;quot;foo&amp;quot;&lt;/code>&lt;/li>
&lt;li>Create a new number value &lt;code>1&lt;/code>&lt;/li>
&lt;li>Create a new Object and store the above key/value pair in it (which involves hashing and finding a bucket)&lt;/li>
&lt;li>Call the &lt;code>load&lt;/code> function.&lt;/li>
&lt;li>Find a variable named &lt;code>x&lt;/code> (note that because it&amp;rsquo;s not defined in the current function sometimes we might actually does have to &amp;ldquo;look farther&amp;rdquo; to notice &lt;code>x&lt;/code> is a global variable).&lt;/li>
&lt;li>Lookup the value of the &lt;code>foo&lt;/code> property on &lt;code>x&lt;/code>. This could involve hashing the string &lt;code>&amp;quot;foo&amp;quot;&lt;/code> to some number to get a bucket to seach for a value.&lt;/li>
&lt;li>Return that value &lt;code>1&lt;/code>&lt;/li>
&lt;li>Call &lt;code>console.log()&lt;/code> with the value &lt;code>1&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>I&amp;rsquo;m intentionally trying to omit some boring details while giving you an idea of the work the CPU is doing. What&amp;rsquo;s important if we&amp;rsquo;re trying to optimize the performance of &lt;code>load&lt;/code>
is to notice that we&amp;rsquo;re probably doing alot of work hashing the foo key to find where &lt;code>1&lt;/code> is stored. But we&amp;rsquo;re smart and we looked at the code and decided that we can replace all the above steps
with just the last one : call &lt;code>console.log()&lt;/code> with the value &lt;code>1&lt;/code>. And that is why in the &lt;a href="https://www.recompiled.dev/blog/deopt/">referenced blog post&lt;/a> the author points out that the runtime debug information printed out:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>0x280008150 &lt;span style="color:#ae81ff">130&lt;/span> d2800040 movz x0, &lt;span style="color:#75715e">#0x2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>// The value is 0x2, and not 0x1 because it&lt;span style="color:#960050;background-color:#1e0010">&amp;#39;&lt;/span>s been tagged.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Don&amp;rsquo;t worry about anything but 0x2. That statement is essentially the proof that the optimizer reduced our function call and our object property lookup
down to just the value &lt;code>1&lt;/code>. Javascript uses something called tagged numbers so you&amp;rsquo;ll have to trust me that in this case 2 on the computer means one in javascript. It&amp;rsquo;s part
of how javascript distinguishes between a number and an &lt;code>Object&lt;/code> at a certain RAM index.&lt;/p>
&lt;p>Let&amp;rsquo;s also look into what assumptions/observations the runtime made in order to make the assumption that
&lt;code>console.log(load())&lt;/code> is &lt;code>console.log(1)&lt;/code>. This is not an exhaustive list:&lt;/p>
&lt;ol>
&lt;li>We have to assume that &lt;code>load()&lt;/code> invokes the load function defined above, it can&amp;rsquo;t have been overridden.&lt;/li>
&lt;li>We have to assume that nothing has changed x.foo (the const means x is always the same object, not that the object is immutable)&lt;/li>
&lt;li>We have to assume nobody has messed with the object&amp;rsquo;s prototype using &lt;code>x.__defineGetter__(&amp;quot;foo&amp;quot;, function() { return 2})&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>And there are some more dependencies that are an artifact of how the runtime implements optimization.&lt;/p>
&lt;h3 id="causing-a-deopt-by-changing-something-that-looked-constant">Causing a deopt by changing something that looked constant&lt;/h3>
&lt;p>So back to reading &amp;ldquo;side effecting a deopt&amp;rdquo;: Now we understand why &lt;code>x.foo=5&lt;/code> causes deoptimization/reoptimization.
What&amp;rsquo;s fascinating is that in their examples v8 didn&amp;rsquo;t fall back to a completely unoptimized path. Instead
of assuming load always returned 1, is fell back to &lt;code>LoadTaggedField(0xc, decompressed)&lt;/code>. I&amp;rsquo;m going
to be a little lazy and not dive deep into the implementation here, and instead assume that means
the code is essentially doing is analogous to what we&amp;rsquo;ve described as modeling the data for x as record
who&amp;rsquo;s with one integer field. Let&amp;rsquo;s write a human readable description of what the runtime has now optimized our function to.
I&amp;rsquo;ll include an offset called foo_offset just like we did above
for person and age. I&amp;rsquo;m also
going to invent a new function. Typically in C this is called &lt;code>&amp;amp;&lt;/code> for &amp;ldquo;address of&amp;rdquo; (or RAM index)
I&amp;rsquo;ll call it ram_index().
So a psuedocode version of the new optimized code is below: we essentially load the foo property
from a fixed offset of the RAM index where x is stored.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#a6e22e">foo_offset&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">console&lt;/span>.&lt;span style="color:#a6e22e">log&lt;/span>(&lt;span style="color:#a6e22e">RAM&lt;/span>[&lt;span style="color:#a6e22e">ram_index&lt;/span>(&lt;span style="color:#a6e22e">foo&lt;/span>)&lt;span style="color:#f92672">+&lt;/span>&lt;span style="color:#a6e22e">foo_offset&lt;/span>]);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// For the c folks
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// console.log(*(&amp;amp;(foo+foo_offset)))
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="expando">Expando&lt;/h3>
&lt;p>We can now pretty easily talk about the effects of taking our optimized value x and adding new properties:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">x&lt;/span>.&lt;span style="color:#a6e22e">__expando&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This causes a deoptimization because the compiler went through all this trouble to figure out that
x had just one property that was a number. Now it has 2. So it has to decide whether it wants to
create a new optimized storage layout for x with just 2 numbers squished together or whether it
should hold off on optimizing. There&amp;rsquo;s no one right answer, but clearly the previous work was invalidated.&lt;/p>
&lt;h3 id="spooky-action-at-a-distance">Spooky action at a distance&lt;/h3>
&lt;p>The last example in the blog post is a fun one. They took our existing optimized code and added a new variable:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-js" data-lang="js">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#a6e22e">y&lt;/span> &lt;span style="color:#f92672">=&lt;/span> {&lt;span style="color:#a6e22e">foo&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>};
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">y&lt;/span>.&lt;span style="color:#a6e22e">__expando&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">4&lt;/span>;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What&amp;rsquo;s surprising is this code causes x to become deoptimized! There&amp;rsquo;s no fundamental reason
why this has to be true but the short answer is that when the runtime optimizes these variables
it essentially has to store a schema (or hidden class) somewhere that says &amp;ldquo;foo is an integer property at at offset 0&amp;rdquo;.&lt;/p>
&lt;p>When the runtime sees &lt;code>y={foo:1}&lt;/code> is does some deduplication so x and y both share the same hidden class/schema. There&amp;rsquo;s now a dependency here
which is that&amp;hellip;. x and y have the same properties.&lt;/p>
&lt;p>When we add a new property to y, we break that assumption and x and y can no longer share the same hidden class. There
are lots of choices about what the runtime could do, and sorting that out results in another
round of deoptimization/reoptimization.&lt;/p>
&lt;p>Let&amp;rsquo;s draw a dependency graph showing the hidden class before/after.&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/ox-hugo/deps.png" />
&lt;/figure>
&lt;h3 id="wrapping-up">Wrapping up&lt;/h3>
&lt;p>This is far from an exhausive post, I&amp;rsquo;m not even sure it was worth writing but so often things like compilers and jits are seen as magic,
which to be honest they kind of are, and I&amp;rsquo;m hopeful that this post helps people unfamiliar with C or jits start to build a mental model
of how the computer can map their code and data into something it understands.&lt;/p>
&lt;p>There&amp;rsquo;s no particular action item to take away. No simple way to make your code fast. In fact javascript runtimes contain so many
heuristics and tricks that sometimes the code that seems like it should run faster runs slower (for example there are many optimizations around
string building and the runtimes can often use fancy datastructures like ropes to speed things up).&lt;/p>
&lt;p>If there&amp;rsquo;s one performance takeaway I would give to the audience of this post, it&amp;rsquo;s this:&lt;/p>
&lt;p>Use a profiler. v8 supports writing symbol maps for linux perf to consume. Chrome has a ton of debug tools. Firefox has great trace viewing
infrastructure. Put your application under a magnifying class and try to understand where it&amp;rsquo;s spending time. Even if your application is faster
than it needs to be it&amp;rsquo;s still nice to know. Is it parsing JSON? Interacting with the DOM? Get a baseline so that as you collect new profiles and develop new features you can spot weird regressions.&lt;/p>
&lt;p>And ask yourself if that makes sense. At my current job alot of programs spend alot of time parsing protocol buffers. This is fairly reasonable. At previous
jobs our caching edge ingress proxies spent alot of time gzipping uncacheable content and doing TLS. This was somewhat reasonable but pointed to opportunities
to improve the customer experience by making more content cacheable.&lt;/p>
&lt;p>Hopefully you found some of this informative, now go out there and put an application under a magnifying glass. I guarantee you&amp;rsquo;ll be surprised at what you find.&lt;/p></content></item><item><title>CGO Performance In Go 1.21</title><link>https://shane.ai/posts/cgo-performance-in-go1.21/</link><pubDate>Fri, 01 Sep 2023 14:42:00 -0700</pubDate><guid>https://shane.ai/posts/cgo-performance-in-go1.21/</guid><description>Tl;Dr Cgo calls take about 40ns, about the same time encoding/json takes to parse a single digit integer. On my 20 core machine Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.
Disclaimer While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve maintained production applications that use Cgo and non-trivial bindings to lua.</description><content>&lt;h2 id="tl-dr">Tl;Dr&lt;/h2>
&lt;p>Cgo calls take about 40ns, about the same time &lt;code>encoding/json&lt;/code> takes to parse a single digit integer. On my 20 core machine
Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.&lt;/p>
&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve
maintained production applications that use Cgo and non-trivial bindings to lua. Performance was great. Go upgrades were a regular source of toil.
The drawback to using Cgo is losing Go cross compilation benefits and having to manage c dependencies. These days
I mainly use Cgo for compatibility and to access libraries that happen to be written in C/C++.&lt;/p>
&lt;h2 id="cgo-and-performance">Cgo &amp;amp; performance&lt;/h2>
&lt;p>Cgo performance is poorly understood in Go, and searching for information online
mixes content from 2 years ago with content from 7 years ago. Cockroach labs wrote a &lt;a href="https://www.cockroachlabs.com/blog/the-cost-and-complexity-of-cgo/">great article&lt;/a>
that measured performance and touched on the complexities of using Cgo. Since then Go performance has improved quite a but, but everything else they said is relevant.
My similar benchmarks are 17x faster than what Cockroach labs saw in 2015. Some of that might be hardware by suspect most of it is just improvements
to Go. Unfortunately I see alot of Go programmers have internalized that &amp;ldquo;Cgo is slow&amp;rdquo; without really knowing
what it&amp;rsquo;s slow compared to. Cgo is slow compared to a regular function call. It&amp;rsquo;s certainly not slow
compared to doing any sort of I/O or parsing work.&lt;/p>
&lt;p>In this post
I want to build on the idea of &amp;ldquo;&lt;a href="https://gist.github.com/jboner/2841832">latency numbers every programmer should know&lt;/a>&amp;rdquo; to figure
out where in the context of &amp;ldquo;slow&amp;rdquo; Cgo lands in the heirarchy of
L1 cache reference -&amp;gt; mutex lock -&amp;gt; main memory reference -&amp;gt; sending a packet on the network. These numbers
are from 2012 so they are really just here to give us a sense of scale:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>latency comparison numbers&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>L1 cache reference&lt;/td>
&lt;td>0.5 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Branch mispredict&lt;/td>
&lt;td>5 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>L2 cache reference&lt;/td>
&lt;td>7 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mutex lock/unlock&lt;/td>
&lt;td>25 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Main memory reference&lt;/td>
&lt;td>100 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Compress 1K bytes with Zippy&lt;/td>
&lt;td>3,000 ns 3 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Send 1K bytes over 1 Gbps network&lt;/td>
&lt;td>10,000 ns 10 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Read 4K randomly from SSD*&lt;/td>
&lt;td>150,000 ns 150 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Read 1 MB sequentially from memory&lt;/td>
&lt;td>250,000 ns 250 us&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Round trip within same datacenter&lt;/td>
&lt;td>500,000 ns 500 us&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>My thesis is: Cgo is has overhead, but it doesn&amp;rsquo;t have as much overhead as it used to and it
may not have as much overhead as you think.&lt;/p>
&lt;p>Lets talk about what Cgo is and a teeny bit about how it works. Cgo
is essentially Go&amp;rsquo;s ffi. When you use Cgo you can call C functions from Go and
pass information back and forth (subject to some &lt;a href="https://pkg.go.dev/cmd/cgo">rules&lt;/a>). The Go compiler autogenerates some
functions to bridge between the Go &amp;amp; C and handle
things like differences in platform calling conventions. There are also mismatches in how blocking
calls are handled and how stack is allocated that make it impractical/unsafe to run Go and C
code on the same stack. I won&amp;rsquo;t go too much into the implementation but at a high level &amp;ldquo;Cgo means IPC between threads&amp;rdquo;
is a good mental model.&lt;/p>
&lt;h2 id="benchmarking">Benchmarking&lt;/h2>
&lt;p>Lets write some benchmarks to explore performance . You can follow along at &lt;a href="https://github.com/shanemhansen/cgobench">github.com/shanemhansen/cgobench&lt;/a>. The code in the repo is autogenerated
from the &lt;a href="https://github.com/shanemhansen/shane.ai/blob/main/docs/content-org/all-posts.org">source org&lt;/a> file for this article using an implementation of Knuth&amp;rsquo;s literate programming. Is it the most productive way to write articles?
Probably not, but it&amp;rsquo;s fun and frankly playing around with new workflows helps my ADHD brain focus. But I digress.&lt;/p>
&lt;p>First off we&amp;rsquo;ll put a no-op go function in &lt;code>bench.go&lt;/code> and do a parallel benchmark. It doesn&amp;rsquo;t do anything
which is a great place to start.&lt;/p>
&lt;p>&lt;code>bench.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--Call">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">Call&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// do less
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we&amp;rsquo;ll add a simple parallel benchmark helper along with our empty call benchmark. I&amp;rsquo;m going to start out
with something so simple the compiler can inline and then compare that to a non-inlined call. When comparing
Go vs Cgo it&amp;rsquo;s important to realize that the Go compiler can&amp;rsquo;t inline Cgo functions.&lt;/p>
&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkCall">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// helper to cut down on boilerplate
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>, &lt;span style="color:#a6e22e">f&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>()) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">f&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Same as above, but explicitly calling the inlineable Call func.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkEmptyCallInlineable&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">Call&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkEmptyCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span>, &lt;span style="color:#a6e22e">Call&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the case of benchmarking no-ops it&amp;rsquo;s always good to check and make sure your code didn&amp;rsquo;t get completely optimized away.
I tend to just look at the disassembled output in &lt;code>BenchmarkEmptyCall&lt;/code> and sure enough I see a convincing &lt;code>call *%rax&lt;/code> instruction in the assembly. A non dynamic
dispatch version would look like: &lt;code>call foo+0x3&lt;/code> but this version is calling a function who&amp;rsquo;s address is in the rax register.&lt;/p>
&lt;p>Let&amp;rsquo;s compile and examine:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>objdump -S cgobench.test | grep -A15 &lt;span style="color:#e6db74">&amp;#39;^0.*/cgobench.BenchmarkEmptyCall.pbench.func&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>0000000000522920 &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1&amp;gt;:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> b.RunParallel(func(pb *testing.PB) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522920: 49 3b 66 10 cmp 0x10(%r14),%rsp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522924: 76 36 jbe 52295c &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x3c&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522926: 55 push %rbp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522927: 48 89 e5 mov %rsp,%rbp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52292a: 48 83 ec 18 sub $0x18,%rsp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52292e: 48 89 44 24 10 mov %rax,0x10(%rsp)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522933: 48 8b 4a 08 mov 0x8(%rdx),%rcx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522937: 48 89 4c 24 08 mov %rcx,0x8(%rsp)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> for pb.Next() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52293c: eb 0f jmp 52294d &amp;lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x2d&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> f()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 52293e: 48 8b 54 24 08 mov 0x8(%rsp),%rdx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522943: 48 8b 02 mov (%rdx),%rax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 522946: ff d0 call *%rax
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we&amp;rsquo;ve verified our benchmark we can run it. I&amp;rsquo;m going to run benchmarks with a few different coure count values so we can see how the output changes. While writing this
post I experimented with some other values and for most benchmarks performance increased linearly with core count up to 16 before it began falling off.
On my machine with 20 cores the overhead of the dynamic call is around 1ns and the inlinable version is significantly faster. As
expected.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,2,4,8,16 -bench EmptyCall
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable 1000000000 0.2784 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-2 1000000000 0.1383 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-4 1000000000 0.07377 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-8 1000000000 0.04089 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCallInlineable-16 1000000000 0.02481 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall 718694536 1.665 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-2 1000000000 0.8346 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-4 1000000000 0.4443 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-8 1000000000 0.2385 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkEmptyCall-16 1000000000 0.1399 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 3.819s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So now I can think of &amp;ldquo;go function call&amp;rdquo; cost as &amp;ldquo;a little more expensive than a L1 cache reference&amp;rdquo; in the above table.
What happens if we add a Cgo call?&lt;/p>
&lt;p>Below is a trivial c function to add 2 integers and a go function to call it. Note that although we might
expect gcc to inline trivial_add, we don&amp;rsquo;t expect Go&amp;rsquo;s compiler to. I did play with some even simpler
C functions but they didn&amp;rsquo;t really perform better.&lt;/p>
&lt;p>&lt;code>bench.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--cgoDefinition">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">trivial_add&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> a, &lt;span style="color:#66d9ef">int&lt;/span> b) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> a&lt;span style="color:#f92672">+&lt;/span>b;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a id="code-snippet--CallCgo">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// wow this is easy
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// import &amp;#34;C&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">CgoCall&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">C&lt;/span>.&lt;span style="color:#a6e22e">trivial_add&lt;/span>(&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkCgo">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkCgoCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">pbench&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span>, &lt;span style="color:#a6e22e">CgoCall&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We run benchmarks in the usual way. Single threaded Cgo overhead is about 40ns. Performance
seems to scale linearly with the number of cores up to 16ish so if I had a Cgo-bound workload I might not
bother putting it on a machine with 32 core, but real workloads usually involve more than just calling a cgo func. We can see:&lt;/p>
&lt;ul>
&lt;li>Cgo has 40ns overhead. That sits somewhere between &amp;ldquo;mutex lock&amp;rdquo; and &amp;ldquo;main memory reference&amp;rdquo;.&lt;/li>
&lt;li>40ns/op is 25 million ops/s. That&amp;rsquo;s pretty good for most projects I&amp;rsquo;ve worked on. At 4ns/op and 16 cores we&amp;rsquo;re getting 250 million ops/s.&lt;/li>
&lt;/ul>
&lt;!--listend-->
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,2,4,8,16,32 -bench Cgo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall 28711474 38.93 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-2 60680826 20.30 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-4 100000000 10.46 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-8 198091461 6.134 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-16 248427465 4.949 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-32 256506208 4.328 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 8.609s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now I want to understand a little bit more about why performance is that way. We&amp;rsquo;ll use Go&amp;rsquo;s great profiling tools to get a better picture of performance at higher core counts. I&amp;rsquo;m a fan of the pprof web view,
which tells us that &lt;code>runtime.(*timeHistorgram).record&lt;/code> and &lt;code>runtime.casgstatus&lt;/code> are taking lots of time. This
tracks with &lt;a href="https://groups.google.com/g/golang-dev/c/XSkrp1_FdiU?pli=1">Ian Lance Taylor&amp;rsquo;s observations&lt;/a>. Interestingly he doesn&amp;rsquo;t expect these operations to be contended,
so there&amp;rsquo;s potential for improving performance.&lt;/p>
&lt;p>Running the test and collecting results:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./cgobench.test -test.cpuprofile&lt;span style="color:#f92672">=&lt;/span>c.out -test.cpu&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">16&lt;/span> -test.bench Cgo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go tool pprof -png cgobench.test c.out &amp;gt; cpu.png
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkCgoCall-16 235322289 4.955 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note the 2 large boxes near the bottom:&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/cpu.png" style="width:50%" />
&lt;/figure>
&lt;p>I also use linux &lt;code>perf&lt;/code>. It does a good job of being able to profile cross language stuff for compiled languages as well as combining both userspace and kernel performance info.
A quick snapshot of (one of) the hot instructions in question in question from perf:&lt;/p>
&lt;figure class="left" >
&lt;img src="https://shane.ai/casgstatus.png" />
&lt;/figure>
&lt;p>Before we put it all together I&amp;rsquo;ll add one final piece of data in to help us get perspective. Here&amp;rsquo;s a carefully crafted JSON decoding benchmark that just parses an integer. It&amp;rsquo;s
written using &lt;code>json.NewDecoder&lt;/code> because just &lt;code>json.Unmarshal&lt;/code> allocates too much. What you&amp;rsquo;ll see below is that a Cgo call is 20% cheaper than a trivial JSON parse using the standard
library in both single threaded and parallel tests.&lt;/p>
&lt;p>&lt;code>bench_test.go&lt;/code>&lt;/p>
&lt;p>&lt;a id="code-snippet--BenchmarkJSON">&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkJSONCall&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">msg&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#e6db74">`1`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">dst&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">r&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">strings&lt;/span>.&lt;span style="color:#a6e22e">NewReader&lt;/span>(&lt;span style="color:#a6e22e">msg&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">dec&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">json&lt;/span>.&lt;span style="color:#a6e22e">NewDecoder&lt;/span>(&lt;span style="color:#a6e22e">r&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">r&lt;/span>.&lt;span style="color:#a6e22e">Seek&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#a6e22e">io&lt;/span>.&lt;span style="color:#a6e22e">SeekStart&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">dec&lt;/span>.&lt;span style="color:#a6e22e">Decode&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>&lt;span style="color:#a6e22e">dst&lt;/span>); &lt;span style="color:#a6e22e">err&lt;/span> &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#66d9ef">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> panic(&lt;span style="color:#a6e22e">err&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go test -cpu&lt;span style="color:#f92672">=&lt;/span>1,16 -bench JSON
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>goos: linux
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>goarch: amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pkg: github.com/shanemhansen/cgobench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkJSONCall 21399691 52.79 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>BenchmarkJSONCall-16 217874599 5.471 ns/op
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PASS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok github.com/shanemhansen/cgobench 2.942s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>So at this point we&amp;rsquo;ve measured performance overhead of Cgo, at least
in terms of wall clock time (note that we haven&amp;rsquo;t looked at memory/thread count/battery usage/etc). We know that the overhead is on the order of 2 mutex operations and that it does
scale with number of cores up to around 16. We&amp;rsquo;ve also seen that with 16 cores we can do around 4ns/op or close to 250 million Cgo ops/s. So if I was looking at using Cgo in 2023 I&amp;rsquo;d definitely use
it outside of very hot loops. There&amp;rsquo;s many reasons I wouldn&amp;rsquo;t use Cgo in 2023 (see disclaimer), but performance is unlikely to be one of them.&lt;/p>
&lt;p>I&amp;rsquo;ll end with this little Cgo version of &amp;ldquo;latency numbers every programmer should know&amp;rdquo; table:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Go/Cgo latency&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Benchmark Name&lt;/td>
&lt;td>1 core&lt;/td>
&lt;td>16 cores&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Inlined Empty func&lt;/td>
&lt;td>0.271 ns&lt;/td>
&lt;td>0.02489 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Empty func&lt;/td>
&lt;td>1.5s ns&lt;/td>
&lt;td>0.135 ns&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>cgo&lt;/code>&lt;/td>
&lt;td>&lt;code>40 ns&lt;/code>&lt;/td>
&lt;td>&lt;code>4.281 ns&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>encoding/json int parse&lt;/td>
&lt;td>52.89 ns&lt;/td>
&lt;td>5.518 ns&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></content></item><item><title>Threads and Goroutines</title><link>https://shane.ai/posts/threads-and-goroutines/</link><pubDate>Mon, 12 Jun 2023 15:33:25 -0700</pubDate><guid>https://shane.ai/posts/threads-and-goroutines/</guid><description>So after several years of reading oversimplified and flat out incorrect comments about threads and fibers/goroutines/async/etc and fighting this reaction:
I&amp;rsquo;ve decided to write my own still-over-simplified all in one guide to the difference between a couple popular threads and fiber implementations. In order to keep this a blog post and not a novel I&amp;rsquo;m just going to focus on linux threads, go goroutines, and rust threads.
tl;dr - Rust threads on linux use 8kb of memory, Goroutines use 2kb.</description><content>&lt;p>So after several years of reading oversimplified and flat out incorrect comments about threads and fibers/goroutines/async/etc and fighting this reaction:&lt;/p>
&lt;p>&lt;img src="https://imgs.xkcd.com/comics/duty_calls.png" alt="Someone on the internet is wrong">&lt;/p>
&lt;p>I&amp;rsquo;ve decided to write my own still-over-simplified all in one guide to the difference between a couple popular threads and fiber implementations. In order to keep this a blog post
and not a novel I&amp;rsquo;m just going to focus on linux threads, go goroutines, and rust threads.&lt;/p>
&lt;p>tl;dr - Rust threads on linux use 8kb of memory, Goroutines use 2kb. It&amp;rsquo;s a big difference but nowhere near as big as the &amp;ldquo;kilobytes vs megabytes&amp;rdquo; claim I often see repeated. The magic of goroutines, if it exists, is
tied up in how those tasks are integration with non-blocking I/O in a userspace scheduler.&lt;/p>
&lt;p>I&amp;rsquo;d like to give you better tools to reason about systems engineering questions like &amp;ldquo;should we use one thread per client?&amp;rdquo; &amp;ldquo;do we need to be async to scale?&amp;rdquo; &amp;ldquo;what concurrency architecture should I choose for my next project?&amp;rdquo;&lt;/p>
&lt;p>Let&amp;rsquo;s start with an example because the rest of the article will essentially discuss these results. We&amp;rsquo;ll talk about whether or not they are surprising and the tradeoffs necessary to get them. So the first question is:&lt;/p>
&lt;h2 id="how-heavy-are-threads">How heavy are threads?&lt;/h2>
&lt;p>I first want to look at how much memory a thread uses. You can find this out simply enough on linux via &lt;code>ulimit&lt;/code> It&amp;rsquo;s changeable. Run this on your favorite linux machine to see what you get. As you can see I get 8 megabytes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ ulimit -a | grep stack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>stack size &lt;span style="color:#f92672">(&lt;/span>kbytes, -s&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#ae81ff">8192&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now this number is correct, but it&amp;rsquo;s often misinterpreted. Now you might think that to make a new thread I need 8 megabytes of RAM free. But thanks to the magic of virtual memory and overcommit, that&amp;rsquo;s not necessarily the case.
The right way to think about this is that the OS, let&amp;rsquo;s assume 64bit, is going to allocate you your own private range but this doesn&amp;rsquo;t really have to be backed by anything. There are alot of 8mb blocks in a 64bit address space.
However there is some book keeping overhead as the kernel tracks it&amp;rsquo;s IOUs.&lt;/p>
&lt;p>Let&amp;rsquo;s write a trivialish program in rust to allocate a million threads and measure the resident memory. But before we do that,
you might have to bump up a couple limits on your system to get the program to run. Here&amp;rsquo;s whad I had to do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>sysctl -w vm.max_map_count&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">4000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sysctl -w kernel.threads-max&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2000000000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I wrote this simple rust program to allocate a million threads that sleep for 1 second, and then wait for all of them.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-rust" data-lang="rust">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">use&lt;/span> std::thread;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">use&lt;/span> std::time::Duration;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">fn&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">let&lt;/span> count &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1_000_000&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">let&lt;/span> &lt;span style="color:#66d9ef">mut&lt;/span> handles &lt;span style="color:#f92672">=&lt;/span> Vec::with_capacity(count);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> _ &lt;span style="color:#66d9ef">in&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">..&lt;/span>count {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> handles.push(thread::spawn(&lt;span style="color:#f92672">||&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> thread::sleep(Duration::from_millis(&lt;span style="color:#ae81ff">1000&lt;/span>));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> handle &lt;span style="color:#66d9ef">in&lt;/span> handles {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> handle.join().unwrap();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s run it and see how it performs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cargo build --release
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/usr/bin/time ./target/release/threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6.17user 80.41system 0:38.55elapsed 224%CPU &lt;span style="color:#f92672">(&lt;/span>0avgtext+0avgdata 8500640maxresident&lt;span style="color:#f92672">)&lt;/span>k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0inputs+0outputs &lt;span style="color:#f92672">(&lt;/span>0major+2125114minor&lt;span style="color:#f92672">)&lt;/span>pagefaults 0swaps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So for those aren&amp;rsquo;t used to reading the somewhat cryptic output of /usr/bin/time, here&amp;rsquo;s how to look at it:&lt;/p>
&lt;ol>
&lt;li>6s of user time: so all the rust code creating/sleeping/etc took 6s.&lt;/li>
&lt;li>80s system time over 38s elapsed time. Which basically says we kept 2 cores busy for 38s and much of the work was in the kernel.&lt;/li>
&lt;li>8500640maxresidentk -&amp;gt; 8.5GB of RAM actually used. Divide that by a million threads and you get about 8KB overhead per thread/stack. That&amp;rsquo;s not too shabby for a &amp;ldquo;heavyweight&amp;rdquo; thread.&lt;/li>
&lt;/ol>
&lt;p>Virtual memory (as scientifically observed by watching &lt;code>top&lt;/code>) peaked at just under 2TB, so about 2MB per thread. How does that square with the 8MB value I said before? I don&amp;rsquo;t know. Maybe rust
passes some flags into &lt;code>clone()&lt;/code> to override the default.&lt;/p>
&lt;p>But there you have it: ignoring kernel structure tracking overhead, simple OS threads that don&amp;rsquo;t do much work use just 8KB of actual RAM on my system. What about Go?&lt;/p>
&lt;h2 id="goroutines-and-stuff">Goroutines and stuff&lt;/h2>
&lt;p>Quick disclaimer: dear pedants: I&amp;rsquo;m aware that a language and a particular implementation are different things and what I&amp;rsquo;m about to say doesn&amp;rsquo;t apply to gccgo. For the rest of this article
&amp;ldquo;Go&amp;rdquo; is both the Go programming language as well as the official Go toolchain.&lt;/p>
&lt;p>With that out of the way: what are goroutines, how do they differ from threads, and how does that make them better or worse?&lt;/p>
&lt;p>From a programmer point of view, a goroutine is basically a thread. It&amp;rsquo;s a function that runs concurrently (and potentially in parallel) with the rest of your program. Executing a function in
a goroutine can allow you to utilize more CPU cores. Go has a M:N threading model which means all your M goroutines are multiplexed over all your N threads (which are then multiplexed over all your CPUs by the kernel). Go defaults to NumThreads==NumCores,
even if you have a million goroutines. With threads you rely on the operating system to switch from one task to another. In Go some of that work happens in userspace. I&amp;rsquo;ll talk more about the details of the differences but first: let&amp;rsquo;s run a the
same &amp;ldquo;one million tasks sleeping for one second&amp;rdquo; test:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">package&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;time&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;sync&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">var&lt;/span> &lt;span style="color:#a6e22e">wg&lt;/span> &lt;span style="color:#a6e22e">sync&lt;/span>.&lt;span style="color:#a6e22e">WaitGroup&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">count&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#ae81ff">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Add&lt;/span>(&lt;span style="color:#a6e22e">count&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">i&lt;/span>&lt;span style="color:#f92672">:=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>;&lt;span style="color:#a6e22e">i&lt;/span>&amp;lt;&lt;span style="color:#a6e22e">count&lt;/span>;&lt;span style="color:#a6e22e">i&lt;/span>&lt;span style="color:#f92672">++&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">go&lt;/span> &lt;span style="color:#66d9ef">func&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">defer&lt;/span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Done&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">time&lt;/span>.&lt;span style="color:#a6e22e">Sleep&lt;/span>(&lt;span style="color:#a6e22e">time&lt;/span>.&lt;span style="color:#a6e22e">Second&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">wg&lt;/span>.&lt;span style="color:#a6e22e">Wait&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s build it&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go build -o threads main.go
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/usr/bin/time ./threads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>16.66user 0.68system 0:02.44elapsed 709%CPU &lt;span style="color:#f92672">(&lt;/span>0avgtext+0avgdata 2122296maxresident&lt;span style="color:#f92672">)&lt;/span>k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>0inputs+0outputs &lt;span style="color:#f92672">(&lt;/span>0major+529900minor&lt;span style="color:#f92672">)&lt;/span>pagefaults 0swaps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>So right off the bat we see:&lt;/p>
&lt;ol>
&lt;li>16s user time. That&amp;rsquo;s way more than rust. Because the rust example is just a shim making syscalls and go is performing scheduling work in userspace.&lt;/li>
&lt;li>0.68 system time. That&amp;rsquo;s low because Go asks the kernel to do less.&lt;/li>
&lt;li>2_122_296maxresident)k. 2 gigabytes of RAM resident or just 2KB/goroutine!&lt;/li>
&lt;li>My unscientific measurement of virtual memory also reported 2GB.&lt;/li>
&lt;/ol>
&lt;p>In this simple benchmark go is over 10x faster at creating a million threads that do some light scheduling. Memory usage is different, but generally same
order of magnitude. It would be a reasonable assumption to say that a non-trivial program would likely exceed 2KB stack starting size and cause it to grow (that&amp;rsquo;s a thing go can do) and so the real memory
usage of rust &amp;amp; go could converge pretty quickly.&lt;/p>
&lt;p>Putting on my SRE hat right now: if someone told me they wanted to run a service with a million goroutines I&amp;rsquo;d be a little nervous. If they told me they needed to run a service with
a million threads I&amp;rsquo;d be more nervous because of virtual memory overhead and managing sysctls. But today&amp;rsquo;s hardware is up for the challenge.&lt;/p>
&lt;p>So if that&amp;rsquo;s true what&amp;rsquo;s the value of goroutines? I like saving RAM and 2GB is smaller than 8GB, but frankly if I&amp;rsquo;m changing runtimes and languages for better performance I want closer to 10x real world
improvement to justify the cost.&lt;/p>
&lt;p>I get asked this question all the time while teaching Go classes at work. If goroutines are so cheap why can&amp;rsquo;t the kernel just make structures that cheap? If go can get away with small stacks why can&amp;rsquo;t the kernel? Go&amp;rsquo;s
task switching was initially &amp;ldquo;cooperative&amp;rdquo; (technically cooperative but managed by the runtime/compiler not the user) and now it&amp;rsquo;s &amp;ldquo;preemptive&amp;rdquo; so it seems like go has to do basically the same context
switching for goroutines that the kernel does for threads: namely saving/restoring registers.&lt;/p>
&lt;p>I&amp;rsquo;ll be honest: I don&amp;rsquo;t entirely know the answer but I have some clues. Go can get away with allocating smaller stacks because Go has always been able to grow the stack if needed. This
is a capability that is tied to the runtime. Regular programs using the rust Thread api (or clone or libc wrappers) may not have always been able to count on growable stacks. Because Go has a more tightly integrated userspace
scheduler and concurrency primitives sometimes it can context switch with less overhead. For example if one goroutine is writing to a channel and one goroutine is reading to a channel, it&amp;rsquo;s possible go can literally run
the reader and the writer on the same thread and take a fast path where the writer goroutine calls send and that triggers the current thread to switchto the reader goroutine.&lt;/p>
&lt;p>I also suspect (but have no proof) that the Go compiler may be able to be less conservative about register state it saves/restores. The linux kernel has to be potentially be ready for more hostile user code. In practice I think
there might be some ancient legacy registers/flags that the kernel has to handle that the Go compiler doesn&amp;rsquo;t.&lt;/p>
&lt;p>So far I&amp;rsquo;ve made goroutines sound pretty boring. They are like threads but use same order of magnitude of RAM. They occasionally can be scheduled smartly but I haven&amp;rsquo;t presented any evidence they can be scheduled/context switched
more efficiently than regular threads. The biggest real benefit I see is that I can use lots of goroutines without worrying as much about configuring system resources.&lt;/p>
&lt;p>So why do they exist and why are they awesome? The answer is actually simple, but first we have to talk about async I/O. The most scalable network I/O on linux is an asynchronous interface called &lt;code>epoll&lt;/code>. Another feature called &lt;code>io_uring&lt;/code> is shaping up to be the most scalable syscall mechanism on linux but it&amp;rsquo;s hoped Go can just switch to that when the time comes. Because these interfaces are async you don&amp;rsquo;t really block a thread on a &lt;code>.Read()&lt;/code> call. Typically we call these systems event driven and utilize &lt;code>callbacks&lt;/code>, short handler functions, to react to new data being read. Node.js for example uses libuv under the hood to do efficient non-blocking evented I/O. Go also transparently uses non-blocking I/O everywhere and it integrates that I/O scheduling with goroutines. non-blocking I/O where possible plus integration of the event loop into the go scheduler is, to answer our earlier question, the manner in which goroutines can be more efficient than threads and it&amp;rsquo;s how go manages to be pretty good at fast networking. It&amp;rsquo;s possible for a call to .Read() to submit a non-blocking I/O request and then cooperatively switch to the next goroutine much like a &lt;code>async&lt;/code> Rust function but without having the colored function problem that often leads to library bufurication. Javascript avoids this by essentially making everything async and non-blocking. Python &amp;amp; Rust have to juggle async/non-async separately.&lt;/p>
&lt;p>So when you combine goroutines and fully integrated non-blocking I/O that&amp;rsquo;s when you get strong multicore performance and a platform that can &amp;ldquo;cheaply&amp;rdquo; handle a large number of network connections while still avoiding &amp;ldquo;callback hell&amp;rdquo; or the &amp;ldquo;function coloring&amp;rdquo; problem. It&amp;rsquo;s not everyone&amp;rsquo;s desired tradeoff. They&amp;rsquo;ve made C interop more expensive, and calls that can&amp;rsquo;t be made non-blocking have to be done in a threadpool (just like node.js and DNS resolution). But if you want to productively write
some fast network servers, it&amp;rsquo;s a powerful batteries included platform.&lt;/p></content></item><item><title>Load Testing Tips</title><link>https://shane.ai/posts/load-testing-tips/</link><pubDate>Wed, 28 Dec 2022 14:35:59 -0800</pubDate><guid>https://shane.ai/posts/load-testing-tips/</guid><description>Load testing tips Over a decade plus of getting retailers ready for a smooth Black Friday I&amp;rsquo;ve collected a few tips, tricks, and stories related to keeping busy applications online during big events.
In fact there&amp;rsquo;s one simple (not easy!) trick to it: the best way to ensure your website can handle a big event is to have your website handle a big event. That may seem like a tautology, but it&amp;rsquo;s where this post starts and it&amp;rsquo;s where it ends.</description><content>&lt;h1 id="load-testing-tips">Load testing tips&lt;/h1>
&lt;p>Over a decade plus of getting retailers ready for a smooth Black Friday I&amp;rsquo;ve collected a few tips, tricks, and stories
related to keeping busy applications online during big events.&lt;/p>
&lt;p>In fact there&amp;rsquo;s one simple (not easy!) trick to it: the best way to ensure your website can handle a big
event is to have your website handle a big event. That may seem like a tautology, but it&amp;rsquo;s where this post
starts and it&amp;rsquo;s where it ends.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Why should you care what I have to say on load testing? I&amp;rsquo;ve spent a decade so far doing this for
everyone from Walmart to Google and that&amp;rsquo;s left me with a bunch of fun war stories I can package up as &amp;ldquo;best practices&amp;rdquo;. Odds
are some of the edge cases I&amp;rsquo;ve ran into will be something you&amp;rsquo;ll run into too and maybe if we&amp;rsquo;re lucky something here will help you avoid an outage.&lt;/p>
&lt;h2 id="how-not-to-load-test">How &lt;em>not&lt;/em> to load test&lt;/h2>
&lt;p>Load testing seems simple enough, but it&amp;rsquo;s a fractal of emergent complexity. To paraphrase the old saying about
backups: &amp;ldquo;customers don&amp;rsquo;t care about load tests, they care about the application working when they need it&amp;rdquo;. It&amp;rsquo;s surprisingly
easy to create load tests that give results that bear no relation to the user experience. I should know because
I&amp;rsquo;ve written my share of them.&lt;/p>
&lt;p>Here&amp;rsquo;s an example of a basic load test. Despite all the flaws we&amp;rsquo;ll discuss, I often start here due to ease of use.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ wrk -t1 -c &lt;span style="color:#ae81ff">10&lt;/span> -d10 &lt;span style="color:#e6db74">&amp;#39;https://example.com/&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Running 10s test @ https://example.com/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">1&lt;/span> threads and &lt;span style="color:#ae81ff">10&lt;/span> connections
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Thread Stats Avg Stdev Max +/- Stdev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Latency 25.00ms 13.04ms 230.48ms 92.26%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Req/Sec 135.68 49.04 202.00 78.79%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">1341&lt;/span> requests in 10.00s, 644.14KB read
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Non-2xx or 3xx responses: &lt;span style="color:#ae81ff">1341&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Requests/sec: 134.08
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Transfer/sec: 64.41KB
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://github.com/wg/wrk">wrk&lt;/a> is a reasonably good tool for generating HTTP request load. The above code uses one
thread and 10 connections to make requests for 10 seconds. I&amp;rsquo;m going to refer to this example a couple times to show how small
changes to your load testing agent can have a big impact on your results.&lt;/p>
&lt;h3 id="caching">Caching&lt;/h3>
&lt;p>If your application has caching and your load test just hammers one single URL repeatedly, it&amp;rsquo;s very likely you could get
artificially high cache hit rates and low latency. For an ecommerce application there&amp;rsquo;s often a very long tail of low traffic
product page requests that don&amp;rsquo;t have a very high cache hit rate. It&amp;rsquo;s not unusual to have a 10x difference in performance
for cached vs uncached responses. The above &lt;code>wrk&lt;/code> test is a perfect example of that.&lt;/p>
&lt;p>There are various ways to fix this, but they depend on the application and CDN configuration. It&amp;rsquo;s possible to
add cache-control headers to request the server disable a cache (in fact when you Ctrl-F5 refresh this is what your browser does). It&amp;rsquo;s also possible to add cache busting query strings via timestamp. But it&amp;rsquo;s up to you whether you want to test the
cached path, the uncached path, or as is most common: the cached path for your expected cache hit ratio and traffic distribution.&lt;/p>
&lt;h3 id="compression">Compression&lt;/h3>
&lt;p>Doing compression wrong is another very frequent mistake. Most customers who interact with your web app using a browser support gzip compression (some of them support brotli). Overall for text payloads like HTML/JSON/JS/CSS gzip compression provides huge bandwidth savings. In addition many CDNs either store responses gzipped to save space, or they store gzipped and non-gzipped responses separately (the relevant part of the HTTP spec involves the Vary header &lt;code>Vary: Accept-Encoding&lt;/code>). Compression can throw off your test results in 2 ways.&lt;/p>
&lt;ol>
&lt;li>You might be bandwidth limited during your tests if you forget to use a client that supports compression.&lt;/li>
&lt;li>I&amp;rsquo;ve seen people get poor performance when using clients that don&amp;rsquo;t support compression with a CDN that stores a single canonical gzip compressed copy of content, resulting in constant unzipping on the fly.&lt;/li>
&lt;/ol>
&lt;p>Avoiding this is generally as simple as ensuring your client sends the &lt;code>Accept-Encoding: gzip&lt;/code> header. For more advanced tests you might want to simulate multiple clients so that you can test gzip and brotli and ensure that cache forking on the 2 methods isn&amp;rsquo;t reducing your overall cache hit rate.&lt;/p>
&lt;p>During one large load test at Walmart we were both saturating bandwidth in one area and also seeing unexpectedly high CPU utilization on the cache servers. Because they stored cached text content normalized using gzip. It turned out the load agents were not configured to indicate gzip support. Which caused the cache servers to spend lots of CPU unzipping as well as waste lots of bandwidth. What at first seemed like a successful load test (at least from the pov of the team trying to find system limits) was in fact testing the wrong thing.&lt;/p>
&lt;h3 id="connection-management">Connection management&lt;/h3>
&lt;p>Connection management is &amp;ldquo;how requests get mapped onto underlying transports&amp;rdquo;. Are you using HTTP/1.1? HTTP/2? HTTP/3? With or without TLS? Keepalive? The answer affects how much load you&amp;rsquo;ll be able to generate on the applications vs the infrastructure between your client and the app. Here&amp;rsquo;s why those questions matter:&lt;/p>
&lt;h4 id="http-protocol-versions">HTTP Protocol versions&lt;/h4>
&lt;p>In the early days of HTTP servers closed the TCP connection to indicate a response was complete. One connection served one request. The problem was that TCP connections can be expensive to setup (See: &lt;a href="https://developer.mozilla.org/en-US/docs/Glossary/TCP_handshake">TCP Handshake&lt;/a> ). TLS &amp;lt; 1.2 handshakes even more so. To be honest I&amp;rsquo;m not 100% sure about TLS1.3. It gets weird with early data and I&amp;rsquo;m not going to try to go into that because I don&amp;rsquo;t understand it yet.&lt;/p>
&lt;p>Anyways developers quickly came up with a way to reuse connections by sending the &lt;code>Content-Length&lt;/code> response header which allowed a client to know when a response was done. Then a new request could be sent on the connection if both the server and client had sent a &lt;a href="https://en.wikipedia.org/wiki/Keepalive">keep-alive&lt;/a> header. When combined with connection pooling this allowed clients to have several concurrent requests multiplexed onto several TCP connections. This was standardized in HTTP/1.1, along with some fixes for streaming content.&lt;/p>
&lt;p>Until recently HTTP/1.1 was the protocol used by default when a CDN connected to your website origin, although as of writing this blog I see that both Google &lt;a href="https://cloud.google.com/media-cdn/docs/origins">Media CDN&lt;/a> and &lt;a href="https://developers.cloudflare.com/cache/how-to/enable-http2-to-origin/">Cloudflare CDN&lt;/a> support HTTP/2 to origin as well although it appears some other popular CDNs such as Akamai &lt;a href="https://myakamai.force.com/customers/s/question/0D54R00007GkHvvSAF/does-akamai-support-http2-between-edge-to-origin?language=en_US">do not&lt;/a>. HTTP/2 addresses some of the problems of HTTP/1.1 by allowing multiple request/responses to be sent concurrently over the same connection. Although you could send multiple requests with HTTP/1.1 you generally had to wait for a response before you sent a new request (ignoring &lt;a href="https://en.wikipedia.org/wiki/HTTP_pipelining">pipelining&lt;/a> which is rarely used). This leads to a problem called &lt;a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">head of line blocking&lt;/a> wherein delays in processing a single request will hold up all the others. The biggest change in HTTP/2 is that multiple connections are no longer needed and it generally runs over TLS (The TCP only version of HTTP/2 is not as well supported). However it can still suffer from head of line blocking because TCP delivers packets in order so one dropped packet means everything on the multiplexed connection stalls. There&amp;rsquo;s also HTTP/3 which is built on top of UDP which enables &lt;a href="https://calendar.perfplanet.com/2020/head-of-line-blocking-in-quic-and-http-3-the-details/#sec_http3">HTTP/3 to solve&lt;/a> the head of line blocking problem.&lt;/p>
&lt;h4 id="connection-configuration-tips">Connection configuration tips&lt;/h4>
&lt;p>Here&amp;rsquo;s what this means for performance testing:&lt;/p>
&lt;p>If you don&amp;rsquo;t use keepalive you&amp;rsquo;ll create tons of TCP connections. That&amp;rsquo;s good if you want to test your load balancer and find out how well you&amp;rsquo;ve tuned your TCP stack on the load generating machines. In all likelihood you will quickly exhaust your ephemeral ports if you do a naive test w/o keepalive. Personally I&amp;rsquo;d likely use HTTP/1.1 over TLS with keepalive for most application load testing. Ideally you want to use what your customers use which is usually HTTP/2, but HTTP/2 support in load testing tools can be spotty, and depending on your CDN/Load Balancer setup odds are you&amp;rsquo;re speaking HTTP/1.1 to origin anyways.&lt;/p>
&lt;p>You can increase that port range on linux systems as well as allow the kernel to more quickly recycle ports with the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>echo &lt;span style="color:#ae81ff">1024&lt;/span> &lt;span style="color:#ae81ff">65535&lt;/span> &amp;gt; /proc/sys/net/ipv4/ip_local_port_range
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sysctl -w net.ipv4.tcp_tw_reuse &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here&amp;rsquo;s a skeleton of a &lt;code>wrk&lt;/code> command that does keepalive and compression over HTTP/1.1 correctly.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># wrk defaults to keepalive&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>wrk -c $CONNECTION_COUNT -t $THREADS -d $DURATION &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -H &lt;span style="color:#e6db74">&amp;#39;Accept-Encoding: gzip&amp;#39;&lt;/span> &lt;span style="color:#e6db74">&amp;#39;https://website/&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Like everything else in engineering, there are tradeoffs. The single best load test you could do would be to have agents everywhere your customers are making connections and requests exactly like your customers. There are some ways to do that, but just as normal software testing involves running fast unit tests more frequently and slower integration tests less often, a good load testing strategy often involves frequent tests with something like &lt;code>wrk&lt;/code> and less frequent high fidelity tests across multiple regions. Most of the time I want to use just a few connections to hammer the application, but occasionally I want to make sure the TCP side of the stack is up to snuff.&lt;/p>
&lt;h3 id="unrepresentative-client-locations">Unrepresentative client locations&lt;/h3>
&lt;p>This is a really broad one, but what happens is that generally your customers are all over your country or possibly all over the world. Quite often a load test is being run from a single region. This can distort results in all sorts of fun ways such as:&lt;/p>
&lt;h4 id="cdn-pop-overload">CDN Pop Overload&lt;/h4>
&lt;p>Your CDN likely routes customers to the closest PoP (Point of Presence). Your load test could overload a single PoP and leave the rest of the CDN and your datacenters with no traffic. Many engineers accidentally load test CDN hot spot mitigation paths, not their app.&lt;/p>
&lt;h4 id="unbalanced-network-traffic">Unbalanced network traffic&lt;/h4>
&lt;p>It is very possible that if all load is coming from a single region, parts along the way can get overloaded. As an example many load balancers doing some form of &lt;a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP&lt;/a> will direct traffic based on some hash of connection information which can result in hot spots if there aren&amp;rsquo;t enough connections. Similarly intermediate routers can be overloaded. For that reason I recommend using multiple load generating agents.&lt;/p>
&lt;p>From a practical perspective I like to use something like &lt;a href="https://cloud.google.com/run">Cloud Run&lt;/a> because it scales to 0 based on traffic so it&amp;rsquo;s pretty cheap to deploy a container to every single region.&lt;/p>
&lt;p>At one point Walmart load tests seemed to hit a ceiling. We couldn&amp;rsquo;t push load any higher, but paradoxically none of the systems handling the load seemed to be at saturation. We ruled out CPU/RAM and bandwidth to individual machines. We went back and checked our work to verify multi-core scalability and came up with nothing. It was unfortunately a stressful time with many teams grasping at straws to figure out the culprit. One of the most interesting clues was that the ceiling happened at a suspiciously round number. Let&amp;rsquo;s say 4Gbps. After a lot of investigation that probably deserves it&amp;rsquo;s own post I realized that we had 2 circuits coming into our DC (lest you think the fact this happened on-prem means you don&amp;rsquo;t have to worry: customers using the Cloud at large scales have lots of analogous things like interconnects and gateways). Theoretically we should have had 2 20Gbps links and they shouldn&amp;rsquo;t have been saturated. After picking up a phone and calling the guy who was responsible for purchasing those I found out from him that: &amp;ldquo;we just brought the new one online, I wonder if the ISP left it at 2Gbps&amp;rdquo;. Which it turns out was the case. One phone call later and we were back in business. Some fraction of our TCP flows from our load testing agents were going to through the under provisioned link and that was creating artificial back pressure on the load testing agents. It just goes to show the importance of monitoring on all your dependencies physical or virtual. I&amp;rsquo;ve seen the same sort of issues with naively written cloud NAT &amp;ldquo;appliances&amp;rdquo; (linux vms) that lack monitoring of &lt;code>nf_conntrack&lt;/code> table size, resulting in mostly-silent degradation.&lt;/p>
&lt;p>This has informed my performance monitoring philosophy which is an extension of my unit testing philosophy.&lt;/p>
&lt;blockquote>
&lt;p>If there&amp;rsquo;s not a test proving it works, it doesn&amp;rsquo;t.&lt;/p>
&lt;/blockquote>
&lt;p>and&lt;/p>
&lt;blockquote>
&lt;p>If you&amp;rsquo;re not monitoring performance, it&amp;rsquo;s degraded.&lt;/p>
&lt;/blockquote>
&lt;h3 id="unrealistic-traffic">Unrealistic traffic&lt;/h3>
&lt;p>Here&amp;rsquo;s a very common story at large retailers. Someone runs a load test. The app passes with flying colors. Organic traffic begins to ramp up. The website crashes. What happened?&lt;/p>
&lt;p>Well it turns out naive load tests often fail to exercise critical components. Here&amp;rsquo;s just a handful of ways I&amp;rsquo;ve seen this happen:&lt;/p>
&lt;p>The load test was detected as a bot and all those super fast 200 responses were captcha pages. Nobody bothered to verify that the page under test was returning the correct content. I wish I could tell a war story here, but there&amp;rsquo;s probably too many to choose from.&lt;/p>
&lt;blockquote>
&lt;p>If you&amp;rsquo;re not asserting you&amp;rsquo;re getting the expected content, you&amp;rsquo;re not.&lt;/p>
&lt;/blockquote>
&lt;p>The load test pulled down a product page (eg &lt;code>https://example.com/product/123&lt;/code>) but since this is a SPA (Single Page App) all the important API calls happened via &lt;code>fetch&lt;/code>/&lt;code>XHR&lt;/code> and so all the load test really did was pull down an empty shell of a page.&lt;/p>
&lt;p>The load test pulled down all the HTML, but none of the associated resources such as images/css/javascript etc. More than once in my career I&amp;rsquo;ve seen developers put timestamp cache busters on their static content, only to cause an outage when they deploy and every single request from customers is a cache miss (eg &lt;code>https://example.com/static/image.jpg?_=$(date +%s)&lt;/code>).&lt;/p>
&lt;h2 id="how-to-create-load-tests-that-give-you-confidence-your-web-app-will-scale">How to create load tests that give you confidence your web app will scale&lt;/h2>
&lt;p>What you want is to simulate a bunch of real customers doing a bunch of realistic things on your web app. Real customers come from different locations. They use different browsers. They look at different products. They do things like search/login/add to chart/checkout/etc. They execute javascript and download images. They use browsers that &lt;a href="https://caniuse.com/http2">support HTTP/2&lt;/a> or &lt;a href="https://caniuse.com/http3">HTTP/3&lt;/a>. Of course running a load test with chrome is a lot more resource intensive than running a load test with &lt;code>wrk&lt;/code>, just like an end to end integration test is more expensive than a unit test.&lt;/p>
&lt;p>I recommend you start out with benchmarks alongside your repo&amp;rsquo;s unit tests. These should be ran frequently and tracked in your metrics system. For every release you should know how the behavior of core endpoints like &lt;code>GET /resource/foo&lt;/code> behaves under load, possibly with mocked data store or API dependencies. Problems found here are cheapest to fix. In Go this looks like something using the &lt;a href="https://pkg.go.dev/net/http/httptest">httptest&lt;/a> package and their &lt;a href="https://pkg.go.dev/testing#hdr-Benchmarks">Benchmark&lt;/a> package. Now it&amp;rsquo;s often the case that the team writing load tests is not the app development team, but SRE best practices show that cross functional teams co-developing features results in fewer big issues during launch. A trivial-ish example might look like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">func&lt;/span> &lt;span style="color:#a6e22e">BenchmarkApp&lt;/span>(&lt;span style="color:#a6e22e">b&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">B&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">b&lt;/span>.&lt;span style="color:#a6e22e">RunParallel&lt;/span>(&lt;span style="color:#66d9ef">func&lt;/span>(&lt;span style="color:#a6e22e">pb&lt;/span> &lt;span style="color:#f92672">*&lt;/span>&lt;span style="color:#a6e22e">testing&lt;/span>.&lt;span style="color:#a6e22e">PB&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">handler&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">MyNewApplicationHandler&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> &lt;span style="color:#a6e22e">pb&lt;/span>.&lt;span style="color:#a6e22e">Next&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">req&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">httptest&lt;/span>.&lt;span style="color:#a6e22e">NewRequest&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;GET&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;http://example.com/foo&amp;#34;&lt;/span>, &lt;span style="color:#66d9ef">nil&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">w&lt;/span> &lt;span style="color:#f92672">:=&lt;/span> &lt;span style="color:#a6e22e">httptest&lt;/span>.&lt;span style="color:#a6e22e">NewRecorder&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">handler&lt;/span>(&lt;span style="color:#a6e22e">w&lt;/span>, &lt;span style="color:#a6e22e">req&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">// validate response recorder `w` returns 200 and correct content
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next layer up is &amp;ldquo;simple&amp;rdquo; scripted CLI load tests. I like &lt;code>wrk&lt;/code> with lua scripting which can do a pretty good job of hitting a bunch of random URLs quickly and defeating caching if needed. It doesn&amp;rsquo;t support http/2 but &lt;a href="https://github.com/tsliwowicz/go-wrk">go-wrk&lt;/a> does. Properly configured with compression support this is a great workhorse for individual teams to test their critical endpoints. You can also do more complicated tests such as requesting a resource and then dependent resources but that can take a lot of work and knowledge of the application to capture the right requests to make. People often use Siege here or even Apache Jmeter.&lt;/p>
&lt;p>Here&amp;rsquo;s an example with &lt;code>wrk&lt;/code> (adapted from &lt;a href="https://medium.com/@felipedutratine/intelligent-benchmark-with-wrk-163986c1587f">Intelligent benchmark with wrk&lt;/a> ) which randomly selects between 1000 product URLs. Lua scripting is very powerful here for creating custom benchmark scenarios.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-lua" data-lang="lua">&lt;span style="display:flex;">&lt;span>math.randomseed(os.time())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>request &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">function&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url_path &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/product/&amp;#34;&lt;/span> &lt;span style="color:#f92672">..&lt;/span> math.random(&lt;span style="color:#ae81ff">0&lt;/span>,&lt;span style="color:#ae81ff">1000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> wrk.format(&lt;span style="color:#e6db74">&amp;#34;GET&amp;#34;&lt;/span>, url_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You would use this script like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>wrk -c10 -t1 -d10s -H &lt;span style="color:#e6db74">&amp;#39;Accept-Encoding: gzip&amp;#39;&lt;/span> -s ./wrk.lua
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally for full scale high fidelity load tests there are relatively few tools out there for browser based load testing. Unfortunately the automation frameworks out there such as selenium can be fragile and have lots of overhead and so &lt;a href="https://www.selenium.dev/documentation/test_practices/discouraged/performance_testing/">they discourage using their libraries for load testing&lt;/a>.&lt;/p>
&lt;p>There are essentially three approaches I like to use.&lt;/p>
&lt;p>The first one feels like cheating but after several years preparing Walmart for Black Friday I can attest to it&amp;rsquo;s unreasonable effectiveness. Before your big event (such as Black Friday) hold some sort of flash sale/small event. For example one year there was a highly anticipated low-quantity device that had just been released. My company stocked a few on their website and sent out an email marketing blast and instantly generated the best load test money can buy as customers flocked to the website to get a good deal. We of course had a minor outage and then had time to optimize and fix before Black Friday/Cyber Monday. If you can run some sort of test marketing event before the big event you absolutely should.&lt;/p>
&lt;p>On the more technical side: it&amp;rsquo;s possible to record a real customer interaction and export HAR (http archive) files. Those can be &lt;a href="https://www.flood.io/blog/convert-har-files-to-jmeter-test-plans">imported into something like jmeter&lt;/a>, or you can write your own converter. I haven&amp;rsquo;t tried this approach yet.&lt;/p>
&lt;p>My preferred approach at the moment is to use headless chrome on cloud run to generate load from multiple regions. With a little scripting in &lt;a href="https://chromedriver.chromium.org/home">chromedriver&lt;/a> it&amp;rsquo;s possible to load a bunch of pages, take screenshots, and export all the timing metrics for later processing. I use &lt;a href="https://cloud.google.com/workflows">Cloud Workflows&lt;/a> to orchestrate the process and take care of ramping up traffic and collecting summary statistics. I&amp;rsquo;m working on open sourcing my workflows/containers here to allow others to easily spin up real-browser load tests. Stay tuned.&lt;/p></content></item></channel></rss>