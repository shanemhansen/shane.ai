<!doctype html><html lang=en><head><title>CGO Performance In Go 1.21 :: Words from Shane</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Tl;Dr Cgo calls take about 40ns, about the same time encoding/json takes to parse a single digit integer. On my 20 core machine Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.
Disclaimer While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve maintained production applications that use Cgo and non-trivial bindings to lua."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://shane.ai/posts/cgo-performance-in-go1.21/><script async src="https://www.googletagmanager.com/gtag/js?id=G-BFC5BJW2FE"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BFC5BJW2FE",{anonymize_ip:!1})}</script><link rel=stylesheet href=https://shane.ai/styles.css><link rel="shortcut icon" href=https://shane.ai/img/theme-colors/pink.png><link rel=apple-touch-icon href=https://shane.ai/img/theme-colors/pink.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="CGO Performance In Go 1.21"><meta property="og:description" content="Tl;Dr Cgo calls take about 40ns, about the same time encoding/json takes to parse a single digit integer. On my 20 core machine Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.
Disclaimer While alot of this article argues that &amp;ldquo;Cgo performance is good actually&amp;rdquo;, please don&amp;rsquo;t take that to mean &amp;ldquo;Cgo is good actually&amp;rdquo;. I&amp;rsquo;ve maintained production applications that use Cgo and non-trivial bindings to lua."><meta property="og:url" content="https://shane.ai/posts/cgo-performance-in-go1.21/"><meta property="og:site_name" content="Words from Shane"><meta property="og:image" content="https://shane.ai/img/favicon/pink.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2023-09-01 14:42:00 -0700 -0700"></head><body class=pink><div class="container headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=https://shane.ai/><div class=logo>Shane.ai</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://shane.ai/posts/cgo-performance-in-go1.21/>CGO Performance In Go 1.21</a></h1><div class=post-meta><time class=post-date>2023-09-01 ::</time>
<span class=post-author>[shane]</span></div><div class=post-content><div><h2 id=tl-dr>Tl;Dr<a href=#tl-dr class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Cgo calls take about 40ns, about the same time <code>encoding/json</code> takes to parse a single digit integer. On my 20 core machine
Cgo call performance scales with core count up to about 16 cores, after which some known contention issues slow things down.</p><h2 id=disclaimer>Disclaimer<a href=#disclaimer class=hanchor arialabel=Anchor>&#8983;</a></h2><p>While alot of this article argues that &ldquo;Cgo performance is good actually&rdquo;, please don&rsquo;t take that to mean &ldquo;Cgo is good actually&rdquo;. I&rsquo;ve
maintained production applications that use Cgo and non-trivial bindings to lua. Performance was great. Go upgrades were a regular source of toil.
The drawback to using Cgo is losing Go cross compilation benefits and having to manage c dependencies. These days
I mainly use Cgo for compatibility and to access libraries that happen to be written in C/C++.</p><h2 id=cgo-and-performance>Cgo & performance<a href=#cgo-and-performance class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Cgo performance is poorly understood in Go, and searching for information online
mixes content from 2 years ago with content from 7 years ago. Cockroach labs wrote a <a href=https://www.cockroachlabs.com/blog/the-cost-and-complexity-of-cgo/>great article</a>
that measured performance and touched on the complexities of using Cgo. Since then Go performance has improved quite a but, but everything else they said is relevant.
My similar benchmarks are 17x faster than what Cockroach labs saw in 2015. Some of that might be hardware by suspect most of it is just improvements
to Go. Unfortunately I see alot of Go programmers have internalized that &ldquo;Cgo is slow&rdquo; without really knowing
what it&rsquo;s slow compared to. Cgo is slow compared to a regular function call. It&rsquo;s certainly not slow
compared to doing any sort of I/O or parsing work.</p><p>In this post
I want to build on the idea of &ldquo;<a href=https://gist.github.com/jboner/2841832>latency numbers every programmer should know</a>&rdquo; to figure
out where in the context of &ldquo;slow&rdquo; Cgo lands in the heirarchy of
L1 cache reference -> mutex lock -> main memory reference -> sending a packet on the network. These numbers
are from 2012 so they are really just here to give us a sense of scale:</p><table><thead><tr><th>latency comparison numbers</th><th></th></tr></thead><tbody><tr><td>L1 cache reference</td><td>0.5 ns</td></tr><tr><td>Branch mispredict</td><td>5 ns</td></tr><tr><td>L2 cache reference</td><td>7 ns</td></tr><tr><td>Mutex lock/unlock</td><td>25 ns</td></tr><tr><td>Main memory reference</td><td>100 ns</td></tr><tr><td>Compress 1K bytes with Zippy</td><td>3,000 ns 3 us</td></tr><tr><td>Send 1K bytes over 1 Gbps network</td><td>10,000 ns 10 us</td></tr><tr><td>Read 4K randomly from SSD*</td><td>150,000 ns 150 us</td></tr><tr><td>Read 1 MB sequentially from memory</td><td>250,000 ns 250 us</td></tr><tr><td>Round trip within same datacenter</td><td>500,000 ns 500 us</td></tr></tbody></table><p>My thesis is: Cgo is has overhead, but it doesn&rsquo;t have as much overhead as it used to and it
may not have as much overhead as you think.</p><p>Lets talk about what Cgo is and a teeny bit about how it works. Cgo
is essentially Go&rsquo;s ffi. When you use Cgo you can call C functions from Go and
pass information back and forth (subject to some <a href=https://pkg.go.dev/cmd/cgo>rules</a>). The Go compiler autogenerates some
functions to bridge between the Go & C and handle
things like differences in platform calling conventions. There are also mismatches in how blocking
calls are handled and how stack is allocated that make it impractical/unsafe to run Go and C
code on the same stack. I won&rsquo;t go too much into the implementation but at a high level &ldquo;Cgo means IPC between threads&rdquo;
is a good mental model.</p><h2 id=benchmarking>Benchmarking<a href=#benchmarking class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Lets write some benchmarks to explore performance . You can follow along at <a href=https://github.com/shanemhansen/cgobench>github.com/shanemhansen/cgobench</a>. The code in the repo is autogenerated
from the <a href=https://github.com/shanemhansen/shane.ai/blob/main/docs/content-org/all-posts.org>source org</a> file for this article using an implementation of Knuth&rsquo;s literate programming. Is it the most productive way to write articles?
Probably not, but it&rsquo;s fun and frankly playing around with new workflows helps my ADHD brain focus. But I digress.</p><p>First off we&rsquo;ll put a no-op go function in <code>bench.go</code> and do a parallel benchmark. It doesn&rsquo;t do anything
which is a great place to start.</p><p><code>bench.go</code></p><p><a id=code-snippet--Call></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>Call</span>() {
</span></span><span style=display:flex><span>	<span style=color:#75715e>// do less
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>}
</span></span></code></pre></div><p>Now we&rsquo;ll add a simple parallel benchmark helper along with our empty call benchmark. I&rsquo;m going to start out
with something so simple the compiler can inline and then compare that to a non-inlined call. When comparing
Go vs Cgo it&rsquo;s important to realize that the Go compiler can&rsquo;t inline Cgo functions.</p><p><code>bench_test.go</code></p><p><a id=code-snippet--BenchmarkCall></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// helper to cut down on boilerplate
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>pbench</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>, <span style=color:#a6e22e>f</span> <span style=color:#66d9ef>func</span>()) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>RunParallel</span>(<span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>pb</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>PB</span>) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>pb</span>.<span style=color:#a6e22e>Next</span>() {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>f</span>()
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#75715e>// Same as above, but explicitly calling the inlineable Call func.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkEmptyCallInlineable</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>RunParallel</span>(<span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>pb</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>PB</span>) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>pb</span>.<span style=color:#a6e22e>Next</span>() {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>Call</span>()
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkEmptyCall</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>pbench</span>(<span style=color:#a6e22e>b</span>, <span style=color:#a6e22e>Call</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>In the case of benchmarking no-ops it&rsquo;s always good to check and make sure your code didn&rsquo;t get completely optimized away.
I tend to just look at the disassembled output in <code>BenchmarkEmptyCall</code> and sure enough I see a convincing <code>call *%rax</code> instruction in the assembly. A non dynamic
dispatch version would look like: <code>call foo+0x3</code> but this version is calling a function who&rsquo;s address is in the rax register.</p><p>Let&rsquo;s compile and examine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go test -c
</span></span><span style=display:flex><span>objdump -S cgobench.test  | grep -A15 <span style=color:#e6db74>&#39;^0.*/cgobench.BenchmarkEmptyCall.pbench.func&#39;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0000000000522920 &lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1&gt;:
</span></span><span style=display:flex><span>	b.RunParallel(func(pb *testing.PB) {
</span></span><span style=display:flex><span>  522920:	49 3b 66 10          	cmp    0x10(%r14),%rsp
</span></span><span style=display:flex><span>  522924:	76 36                	jbe    52295c &lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x3c&gt;
</span></span><span style=display:flex><span>  522926:	55                   	push   %rbp
</span></span><span style=display:flex><span>  522927:	48 89 e5             	mov    %rsp,%rbp
</span></span><span style=display:flex><span>  52292a:	48 83 ec 18          	sub    $0x18,%rsp
</span></span><span style=display:flex><span>  52292e:	48 89 44 24 10       	mov    %rax,0x10(%rsp)
</span></span><span style=display:flex><span>  522933:	48 8b 4a 08          	mov    0x8(%rdx),%rcx
</span></span><span style=display:flex><span>  522937:	48 89 4c 24 08       	mov    %rcx,0x8(%rsp)
</span></span><span style=display:flex><span>		for pb.Next() {
</span></span><span style=display:flex><span>  52293c:	eb 0f                	jmp    52294d &lt;github.com/shanemhansen/cgobench.BenchmarkEmptyCall.pbench.func1+0x2d&gt;
</span></span><span style=display:flex><span>			f()
</span></span><span style=display:flex><span>  52293e:	48 8b 54 24 08       	mov    0x8(%rsp),%rdx
</span></span><span style=display:flex><span>  522943:	48 8b 02             	mov    (%rdx),%rax
</span></span><span style=display:flex><span>  522946:	ff d0                	call   *%rax
</span></span></code></pre></div><p>Now that we&rsquo;ve verified our benchmark we can run it. I&rsquo;m going to run benchmarks with a few different coure count values so we can see how the output changes. While writing this
post I experimented with some other values and for most benchmarks performance increased linearly with core count up to 16 before it began falling off.
On my machine with 20 cores the overhead of the dynamic call is around 1ns and the inlinable version is significantly faster. As
expected.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go test -cpu<span style=color:#f92672>=</span>1,2,4,8,16  -bench EmptyCall
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>goos: linux
</span></span><span style=display:flex><span>goarch: amd64
</span></span><span style=display:flex><span>pkg: github.com/shanemhansen/cgobench
</span></span><span style=display:flex><span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
</span></span><span style=display:flex><span>BenchmarkEmptyCallInlineable       	1000000000	         0.2784 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCallInlineable-2     	1000000000	         0.1383 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCallInlineable-4     	1000000000	         0.07377 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCallInlineable-8     	1000000000	         0.04089 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCallInlineable-16    	1000000000	         0.02481 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCall                 	718694536	         1.665 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCall-2               	1000000000	         0.8346 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCall-4               	1000000000	         0.4443 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCall-8               	1000000000	         0.2385 ns/op
</span></span><span style=display:flex><span>BenchmarkEmptyCall-16              	1000000000	         0.1399 ns/op
</span></span><span style=display:flex><span>PASS
</span></span><span style=display:flex><span>ok  	github.com/shanemhansen/cgobench	3.819s
</span></span></code></pre></div><p>So now I can think of &ldquo;go function call&rdquo; cost as &ldquo;a little more expensive than a L1 cache reference&rdquo; in the above table.
What happens if we add a Cgo call?</p><p>Below is a trivial c function to add 2 integers and a go function to call it. Note that although we might
expect gcc to inline trivial_add, we don&rsquo;t expect Go&rsquo;s compiler to. I did play with some even simpler
C functions but they didn&rsquo;t really perform better.</p><p><code>bench.go</code></p><p><a id=code-snippet--cgoDefinition></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>trivial_add</span>(<span style=color:#66d9ef>int</span> a, <span style=color:#66d9ef>int</span> b) {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> a<span style=color:#f92672>+</span>b;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><a id=code-snippet--CallCgo></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// wow this is easy
</span></span></span><span style=display:flex><span><span style=color:#75715e>// import &#34;C&#34;
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>CgoCall</span>() {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>C</span>.<span style=color:#a6e22e>trivial_add</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>bench_test.go</code></p><p><a id=code-snippet--BenchmarkCgo></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkCgoCall</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>pbench</span>(<span style=color:#a6e22e>b</span>, <span style=color:#a6e22e>CgoCall</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We run benchmarks in the usual way. Single threaded Cgo overhead is about 40ns. Performance
seems to scale linearly with the number of cores up to 16ish so if I had a Cgo-bound workload I might not
bother putting it on a machine with 32 core, but real workloads usually involve more than just calling a cgo func. We can see:</p><ul><li>Cgo has 40ns overhead. That sits somewhere between &ldquo;mutex lock&rdquo; and &ldquo;main memory reference&rdquo;.</li><li>40ns/op is 25 million ops/s. That&rsquo;s pretty good for most projects I&rsquo;ve worked on. At 4ns/op and 16 cores we&rsquo;re getting 250 million ops/s.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go test -cpu<span style=color:#f92672>=</span>1,2,4,8,16,32  -bench Cgo
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>goos: linux
</span></span><span style=display:flex><span>goarch: amd64
</span></span><span style=display:flex><span>pkg: github.com/shanemhansen/cgobench
</span></span><span style=display:flex><span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
</span></span><span style=display:flex><span>BenchmarkCgoCall       	28711474	        38.93 ns/op
</span></span><span style=display:flex><span>BenchmarkCgoCall-2     	60680826	        20.30 ns/op
</span></span><span style=display:flex><span>BenchmarkCgoCall-4     	100000000	        10.46 ns/op
</span></span><span style=display:flex><span>BenchmarkCgoCall-8     	198091461	         6.134 ns/op
</span></span><span style=display:flex><span>BenchmarkCgoCall-16    	248427465	         4.949 ns/op
</span></span><span style=display:flex><span>BenchmarkCgoCall-32    	256506208	         4.328 ns/op
</span></span><span style=display:flex><span>PASS
</span></span><span style=display:flex><span>ok  	github.com/shanemhansen/cgobench	8.609s
</span></span></code></pre></div><p>Now I want to understand a little bit more about why performance is that way. We&rsquo;ll use Go&rsquo;s great profiling tools to get a better picture of performance at higher core counts. I&rsquo;m a fan of the pprof web view,
which tells us that <code>runtime.(*timeHistorgram).record</code> and <code>runtime.casgstatus</code> are taking lots of time. This
tracks with <a href="https://groups.google.com/g/golang-dev/c/XSkrp1_FdiU?pli=1">Ian Lance Taylor&rsquo;s observations</a>. Interestingly he doesn&rsquo;t expect these operations to be contended,
so there&rsquo;s potential for improving performance.</p><p>Running the test and collecting results:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go test -c
</span></span><span style=display:flex><span>./cgobench.test  -test.cpuprofile<span style=color:#f92672>=</span>c.out -test.cpu<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> -test.bench Cgo
</span></span><span style=display:flex><span>go tool pprof -png cgobench.test c.out &gt; cpu.png
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>goos: linux
</span></span><span style=display:flex><span>goarch: amd64
</span></span><span style=display:flex><span>pkg: github.com/shanemhansen/cgobench
</span></span><span style=display:flex><span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
</span></span><span style=display:flex><span>BenchmarkCgoCall-16    	235322289	         4.955 ns/op
</span></span><span style=display:flex><span>PASS
</span></span></code></pre></div><p>Note the 2 large boxes near the bottom:</p><figure class=left><img src=/cpu.png style=width:50%></figure><p>I also use linux <code>perf</code>. It does a good job of being able to profile cross language stuff for compiled languages as well as combining both userspace and kernel performance info.
A quick snapshot of (one of) the hot instructions in question in question from perf:</p><figure class=left><img src=/casgstatus.png></figure><p>Before we put it all together I&rsquo;ll add one final piece of data in to help us get perspective. Here&rsquo;s a carefully crafted JSON decoding benchmark that just parses an integer. It&rsquo;s
written using <code>json.NewDecoder</code> because just <code>json.Unmarshal</code> allocates too much. What you&rsquo;ll see below is that a Cgo call is 20% cheaper than a trivial JSON parse using the standard
library in both single threaded and parallel tests.</p><p><code>bench_test.go</code></p><p><a id=code-snippet--BenchmarkJSON></a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkJSONCall</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>msg</span> <span style=color:#f92672>:=</span> <span style=color:#e6db74>`1`</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>RunParallel</span>(<span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>pb</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>PB</span>) {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>dst</span> <span style=color:#66d9ef>int</span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>r</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>strings</span>.<span style=color:#a6e22e>NewReader</span>(<span style=color:#a6e22e>msg</span>)
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>dec</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>json</span>.<span style=color:#a6e22e>NewDecoder</span>(<span style=color:#a6e22e>r</span>)
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>pb</span>.<span style=color:#a6e22e>Next</span>() {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>r</span>.<span style=color:#a6e22e>Seek</span>(<span style=color:#ae81ff>0</span>, <span style=color:#a6e22e>io</span>.<span style=color:#a6e22e>SeekStart</span>)
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>if</span> <span style=color:#a6e22e>err</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>dec</span>.<span style=color:#a6e22e>Decode</span>(<span style=color:#f92672>&amp;</span><span style=color:#a6e22e>dst</span>); <span style=color:#a6e22e>err</span> <span style=color:#f92672>!=</span> <span style=color:#66d9ef>nil</span> {
</span></span><span style=display:flex><span>				panic(<span style=color:#a6e22e>err</span>)
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go test -cpu<span style=color:#f92672>=</span>1,16 -bench JSON
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>goos: linux
</span></span><span style=display:flex><span>goarch: amd64
</span></span><span style=display:flex><span>pkg: github.com/shanemhansen/cgobench
</span></span><span style=display:flex><span>cpu: 12th Gen Intel(R) Core(TM) i7-12700H
</span></span><span style=display:flex><span>BenchmarkJSONCall       	21399691	        52.79 ns/op
</span></span><span style=display:flex><span>BenchmarkJSONCall-16    	217874599	         5.471 ns/op
</span></span><span style=display:flex><span>PASS
</span></span><span style=display:flex><span>ok  	github.com/shanemhansen/cgobench	2.942s
</span></span></code></pre></div><h2 id=conclusions>Conclusions<a href=#conclusions class=hanchor arialabel=Anchor>&#8983;</a></h2><p>So at this point we&rsquo;ve measured performance overhead of Cgo, at least
in terms of wall clock time (note that we haven&rsquo;t looked at memory/thread count/battery usage/etc). We know that the overhead is on the order of 2 mutex operations and that it does
scale with number of cores up to around 16. We&rsquo;ve also seen that with 16 cores we can do around 4ns/op or close to 250 million Cgo ops/s. So if I was looking at using Cgo in 2023 I&rsquo;d definitely use
it outside of very hot loops. There&rsquo;s many reasons I wouldn&rsquo;t use Cgo in 2023 (see disclaimer), but performance is unlikely to be one of them.</p><p>I&rsquo;ll end with this little Cgo version of &ldquo;latency numbers every programmer should know&rdquo; table:</p><table><thead><tr><th>Go/Cgo latency</th><th></th><th></th></tr></thead><tbody><tr><td>Benchmark Name</td><td>1 core</td><td>16 cores</td></tr><tr><td>Inlined Empty func</td><td>0.271 ns</td><td>0.02489 ns</td></tr><tr><td>Empty func</td><td>1.5s ns</td><td>0.135 ns</td></tr><tr><td><code>cgo</code></td><td><code>40 ns</code></td><td><code>4.281 ns</code></td></tr><tr><td>encoding/json int parse</td><td>52.89 ns</td><td>5.518 ns</td></tr></tbody></table></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://shane.ai/posts/notes-on-javascript-jit-deopt/><span class=button__icon>←</span>
<span class=button__text>Some notes on javascript jit and deopt</span></a></span>
<span class="button next"><a href=https://shane.ai/posts/threads-and-goroutines/><span class=button__text>Threads and Goroutines</span>
<span class=button__icon>→</span></a></span></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2023 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>